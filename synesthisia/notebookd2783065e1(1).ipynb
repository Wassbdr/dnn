{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14542704,"sourceType":"datasetVersion","datasetId":9288379},{"sourceId":14542740,"sourceType":"datasetVersion","datasetId":9288403},{"sourceId":14546904,"sourceType":"datasetVersion","datasetId":9291107},{"sourceId":14547036,"sourceType":"datasetVersion","datasetId":9291185},{"sourceId":14547120,"sourceType":"datasetVersion","datasetId":9291241}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Synesthesia (Kaggle Edition)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup & Config","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport torchaudio.transforms as T\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport random\nimport os\nimport cv2\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\nimport plotly.graph_objects as go\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport shutil\n\n# Environment Detection\nif os.path.exists('/kaggle/input'):\n    print(\"Kaggle Environment Detected\")\n    CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\nelse:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        CHECKPOINT_DIR = \"/content/drive/MyDrive/Synesthesia_Checkpoints\"\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    except:\n        CHECKPOINT_DIR = \"./checkpoints\"\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nIMG_SIZE = 256\nSAMPLE_RATE = 22050\nDURATION = 1.0 \nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 256\nBATCH_SIZE = 32 if torch.cuda.get_device_properties(0).total_memory > 14e9 else 16\nLR = 0.0001\nLAMBDA_L1 = 150\nEPOCHS = 200\nSTART_EPOCH = 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:49:20.279464Z","iopub.execute_input":"2026-01-19T10:49:20.279723Z","iopub.status.idle":"2026-01-19T10:49:20.288401Z","shell.execute_reply.started":"2026-01-19T10:49:20.279701Z","shell.execute_reply":"2026-01-19T10:49:20.287716Z"}},"outputs":[{"name":"stdout","text":"Kaggle Environment Detected\nCheckpoints will be saved to: /kaggle/working/checkpoints\nUsing device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Unified Normalization","metadata":{}},{"cell_type":"code","source":"\nclass SpectrogramNormalizer:\n    @staticmethod\n    def transform(waveform):\n        mel_transform = T.MelSpectrogram(\n            sample_rate=SAMPLE_RATE,\n            n_fft=N_FFT,\n            win_length=N_FFT,\n            hop_length=HOP_LENGTH,\n            n_mels=IMG_SIZE,\n            power=2.0\n        ).to(waveform.device)\n        spec = mel_transform(waveform)\n        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n        spec = (spec + 40) / 40\n        spec = torch.clamp(spec, -1, 1)\n        if spec.dim() == 2:\n            spec = spec.unsqueeze(0)\n        spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n        return spec\nnormalizer = SpectrogramNormalizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:49:20.289509Z","iopub.execute_input":"2026-01-19T10:49:20.289730Z","iopub.status.idle":"2026-01-19T10:49:20.323301Z","shell.execute_reply.started":"2026-01-19T10:49:20.289710Z","shell.execute_reply":"2026-01-19T10:49:20.322749Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 3. Style-Injected Dataset","metadata":{}},{"cell_type":"code","source":"\n# --- STYLE INJECTOR ---\nclass StyleInjector:\n    def __init__(self, size=256, texture_path=\"style.jpg\"):\n        self.size = size\n        self.texture = self.load_or_generate_texture(texture_path)\n    \n    def load_or_generate_texture(self, path):\n        # Check current dir, then Kaggle Input, then Drive\n        possible_paths = [\n            path,\n            f\"/kaggle/input/{path}\",\n            f\"/kaggle/working/{path}\",\n            f\"/content/drive/MyDrive/{path}\"\n        ]\n        \n        found_path = None\n        for p in possible_paths:\n            if os.path.exists(p):\n                found_path = p\n                break\n        \n        if found_path:\n            print(f\"Loading custom style texture from {found_path}\")\n            try:\n                img = Image.open(found_path).convert('RGB').resize((self.size, self.size))\n                return np.array(img).astype(np.float32) / 255.0\n            except Exception as e:\n                print(f\"Error loading texture: {e}\")\n        \n        print(\"Using Procedural Paper Texture fallback.\")\n        return self.generate_paper_texture()\n\n    def generate_paper_texture(self):\n        noise = np.random.normal(0.95, 0.05, (self.size, self.size, 3))\n        for _ in range(20):\n            x1, y1 = np.random.randint(0, self.size, 2)\n            length = np.random.randint(5, 20)\n            angle = np.random.uniform(0, 360)\n            x2 = int(x1 + length * np.cos(np.radians(angle)))\n            y2 = int(y1 + length * np.sin(np.radians(angle)))\n            cv2.line(noise, (x1, y1), (x2, y2), (0.8, 0.8, 0.7), 1)\n        return np.clip(noise, 0, 1).astype(np.float32)\n\n    def apply_style(self, generated_img):\n        return np.clip(generated_img * self.texture, 0, 1).astype(np.float32)\n\n# --- AUDIO & VISUAL GENS ---\nclass AdvancedAudioGenerator:\n    def __init__(self, sample_rate=22050, duration=1.0):\n        self.sr = sample_rate\n        self.duration = duration\n        self.n_samples = int(sample_rate * duration)\n\n    def generate_sine(self, freq=None): \n        if freq is None: freq = random.uniform(200, 1000)\n        t = np.linspace(0, self.duration, self.n_samples)\n        audio = np.sin(2 * np.pi * freq * t)\n        return audio.astype(np.float32), \"sine\", freq\n        \n    def generate_white_noise(self):\n        audio = np.random.normal(0, 0.5, self.n_samples)\n        return audio.astype(np.float32), \"noise\", 0\n        \n    def generate_chirp(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        f0 = random.uniform(100, 400); f1 = random.uniform(800, 1500)\n        k = (f1 - f0) / self.duration\n        audio = np.sin(2 * np.pi * (f0 * t + 0.5 * k * t**2))\n        return audio.astype(np.float32), \"chirp\", (f0, f1)\n\n    def generate_fm(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        cf = random.uniform(200, 800); mf = random.uniform(10, 100); mi = random.uniform(1, 10)\n        audio = np.sin(2 * np.pi * cf * t + mi * np.sin(2 * np.pi * mf * t))\n        return audio.astype(np.float32), \"fm\", (cf, mf)\n\n    def generate_percussive(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        noise = np.random.normal(0, 0.8, self.n_samples)\n        decay = np.exp(-10 * t) \n        audio = noise * decay\n        return audio.astype(np.float32), \"percussive\", 0\n\nclass AdvancedVisualGenerator:\n    def __init__(self, img_size=256):\n        self.size = img_size\n\n    def generate_gradient(self, color_phase=0.0):\n        x = np.linspace(0, 1, self.size); y = np.linspace(0, 1, self.size)\n        X, Y = np.meshgrid(x, y)\n        R = Y; G = np.sin(Y * np.pi + color_phase) * 0.5 + 0.5; B = 1.0 - Y\n        return np.stack([R, G, B], axis=-1).astype(np.float32)\n\n    def generate_noise_texture(self):\n        noise = np.random.uniform(0, 1, (self.size, self.size, 3))\n        noise = np.clip((noise - 0.5) * 2.5 + 0.5, 0, 1) \n        return noise.astype(np.float32)\n\n    def generate_structured_pattern(self):\n        x = np.linspace(0, 10, self.size); y = np.linspace(0, 10, self.size)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(X + Y) * 0.5 + 0.5\n        return np.stack([Z, Z, 1-Z], axis=-1).astype(np.float32)\n\n    def generate_voronoi(self):\n        n_points = 20\n        points = np.random.rand(n_points, 2) * self.size\n        x = np.arange(self.size); y = np.arange(self.size)\n        X, Y = np.meshgrid(x, y)\n        img = np.zeros((self.size, self.size, 3))\n        for px, py in points:\n            dist = np.sqrt((X - px)**2 + (Y - py)**2)\n            img[:, :, 0] += np.exp(-dist * 0.05)\n            img[:, :, 1] += np.exp(-dist * 0.03) * np.sin(px)\n            img[:, :, 2] += np.exp(-dist * 0.04)\n        return np.clip(img, 0, 1).astype(np.float32)\n\n    def generate_fractal_percussive(self):\n        x = np.linspace(-1, 1, self.size); y = np.linspace(-1, 1, self.size)\n        X, Y = np.meshgrid(x, y)\n        R = np.sqrt(X**2 + Y**2); A = np.arctan2(Y, X)\n        val = np.sin(A * 20) * 0.5 + 0.5\n        val *= np.exp(-R * 2)\n        img = np.stack([val, 1-val, np.random.rand(*val.shape)*val], axis=-1)\n        return img.astype(np.float32)\n\nclass SynesthesiaDataset(Dataset):\n    def __init__(self, size=2000, img_size=256, sample_rate=22050, style_file=\"style.jpg\"):\n        self.size = size\n        self.audio_gen = AdvancedAudioGenerator(sample_rate)\n        self.vis_gen = AdvancedVisualGenerator(img_size)\n        self.style_injector = StyleInjector(img_size, style_file)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        choice = random.choice([\"sine\", \"noise\", \"chirp\", \"fm\", \"percussive\"])\n        \n        if choice == \"sine\":\n            audio, _, _ = self.audio_gen.generate_sine()\n            raw_img = self.vis_gen.generate_gradient(random.random())\n        elif choice == \"noise\":\n            audio, _, _ = self.audio_gen.generate_white_noise()\n            raw_img = self.vis_gen.generate_noise_texture()\n        elif choice == \"chirp\":\n            audio, _, _ = self.audio_gen.generate_chirp()\n            raw_img = self.vis_gen.generate_structured_pattern()\n        elif choice == \"fm\":\n            audio, _, _ = self.audio_gen.generate_fm()\n            raw_img = self.vis_gen.generate_voronoi()\n        else: \n            audio, _, _ = self.audio_gen.generate_percussive()\n            raw_img = self.vis_gen.generate_fractal_percussive()\n\n        # APPLY STYLE\n        styled_img_np = self.style_injector.apply_style(raw_img)\n\n        # Process\n        audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n        spec = normalizer.transform(audio_tensor)\n\n        target_img = torch.from_numpy(styled_img_np).permute(2, 0, 1)\n        target_img = (target_img * 2.0) - 1.0 # 0..1 to -1..1\n\n        return spec, target_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:49:20.324056Z","iopub.execute_input":"2026-01-19T10:49:20.324334Z","iopub.status.idle":"2026-01-19T10:49:20.349831Z","shell.execute_reply.started":"2026-01-19T10:49:20.324299Z","shell.execute_reply":"2026-01-19T10:49:20.349262Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 4. Pix2Pix Model","metadata":{}},{"cell_type":"code","source":"\nclass UNetDown(nn.Module):\n    def __init__(self, in_c, out_c, norm=True, drop=0.0):\n        super().__init__()\n        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n        if norm: layers.append(nn.InstanceNorm2d(out_c))\n        layers.append(nn.LeakyReLU(0.2))\n        if drop: layers.append(nn.Dropout(drop))\n        self.model = nn.Sequential(*layers)\n    def forward(self, x): return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_c, out_c, drop=0.0):\n        super().__init__()\n        layers = [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False), nn.InstanceNorm2d(out_c), nn.ReLU(True)]\n        if drop: layers.append(nn.Dropout(drop))\n        self.model = nn.Sequential(*layers)\n    def forward(self, x, skip):\n        x = self.model(x)\n        # Resize if needed (shape safety)\n        if x.shape[2:] != skip.shape[2:]:\n             x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear')\n        return torch.cat((x, skip), 1)\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, in_c=1, out_c=3):\n        super().__init__()\n        self.d1=UNetDown(in_c,64,norm=False); self.d2=UNetDown(64,128); self.d3=UNetDown(128,256)\n        self.d4=UNetDown(256,512,drop=0.5); self.d5=UNetDown(512,512,drop=0.5); self.d6=UNetDown(512,512,drop=0.5)\n        self.d7=UNetDown(512,512,drop=0.5); self.d8=UNetDown(512,512,norm=False,drop=0.5)\n        self.u1=UNetUp(512,512,drop=0.5); self.u2=UNetUp(1024,512,drop=0.5); self.u3=UNetUp(1024,512,drop=0.5)\n        self.u4=UNetUp(1024,512,drop=0.5); self.u5=UNetUp(1024,256); self.u6=UNetUp(512,128); self.u7=UNetUp(256,64)\n        self.final=nn.Sequential(nn.Upsample(scale_factor=2), nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(128,out_c,4,padding=1), nn.Tanh())\n    def forward(self, x):\n        d1=self.d1(x); d2=self.d2(d1); d3=self.d3(d2); d4=self.d4(d3); d5=self.d5(d4); d6=self.d6(d5); d7=self.d7(d6); d8=self.d8(d7)\n        u1=self.u1(d8,d7); u2=self.u2(u1,d6); u3=self.u3(u2,d5); u4=self.u4(u3,d4); u5=self.u5(u4,d3); u6=self.u6(u5,d2); u7=self.u7(u6,d1)\n        return self.final(u7)\n\nclass PatchGANDiscriminator(nn.Module):\n    def __init__(self, in_c=1):\n        super().__init__()\n        def block(i, o, n=True):\n            l = [nn.Conv2d(i, o, 4, 2, 1)]; \n            if n: l.append(nn.InstanceNorm2d(o)); \n            l.append(nn.LeakyReLU(0.2, True)); return l\n        self.model = nn.Sequential(*block(in_c+3,64,False), *block(64,128), *block(128,256), *block(256,512), \n                                   nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(512,1,4,padding=1,bias=False))\n    def forward(self, a, b): return self.model(torch.cat((a, b), 1))\n\ndef weights_init_normal(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight.data, 1.0, 0.02); torch.nn.init.constant_(m.bias.data, 0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:49:20.351006Z","iopub.execute_input":"2026-01-19T10:49:20.351197Z","iopub.status.idle":"2026-01-19T10:49:20.372642Z","shell.execute_reply.started":"2026-01-19T10:49:20.351173Z","shell.execute_reply":"2026-01-19T10:49:20.371994Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 5. Professional Training (Styled)","metadata":{}},{"cell_type":"code","source":"\ndef save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D):\n    path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n    torch.save({'epoch': epoch, 'G': generator.state_dict(), 'D': discriminator.state_dict(), 'optG': optimizer_G.state_dict(), 'optD': optimizer_D.state_dict()}, path)\n    print(f\"Saved: {path}\")\n\ndef load_checkpoint(generator, discriminator, optimizer_G, optimizer_D):\n    files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"checkpoint\")], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n    if not files: return 0\n    # Try catch for corrupted files\n    try:\n        cp = torch.load(os.path.join(CHECKPOINT_DIR, files[-1]), map_location=DEVICE)\n        generator.load_state_dict(cp['G']); discriminator.load_state_dict(cp['D']); optimizer_G.load_state_dict(cp['optG']); optimizer_D.load_state_dict(cp['optD'])\n        print(f\"Resumed epoch {cp['epoch']}\"); return cp['epoch'] + 1\n    except:\n        return 0\n\ngenerator = UNetGenerator().to(DEVICE); discriminator = PatchGANDiscriminator().to(DEVICE)\ngenerator.apply(weights_init_normal); discriminator.apply(weights_init_normal)\ncriterion_GAN = nn.BCEWithLogitsLoss().to(DEVICE); criterion_L1 = nn.L1Loss().to(DEVICE)\noptimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\nscheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=EPOCHS)\nscheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=EPOCHS)\n\ndataloader = DataLoader(SynesthesiaDataset(size=4000, style_file=\"/kaggle/input/stylee/smooth-stucco-wall.jpg\"), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nscaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None\n\nSTART_EPOCH = load_checkpoint(generator, discriminator, optimizer_G, optimizer_D)\n\nfor epoch in range(START_EPOCH, EPOCHS):\n    for i, (spec, real_b) in enumerate(dataloader):\n        real_a, real_b = spec.to(DEVICE), real_b.to(DEVICE)\n        \n        # Train G\n        optimizer_G.zero_grad()\n        with torch.amp.autocast('cuda') if scaler else torch.no_grad():\n             if scaler:\n                fake_b = generator(real_a)\n                pred_fake = discriminator(fake_b, real_a)\n                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n             else:\n                fake_b = generator(real_a)\n                pred_fake = discriminator(fake_b, real_a)\n                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n        \n        if scaler: scaler.scale(loss_G).backward(); scaler.step(optimizer_G); scaler.update()\n        else: loss_G.backward(); optimizer_G.step()\n\n        # Train D\n        optimizer_D.zero_grad()\n        with torch.amp.autocast('cuda') if scaler else torch.no_grad():\n             if scaler:\n                pred_real = discriminator(real_b, real_a)\n                pred_fake = discriminator(fake_b.detach(), real_a)\n                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n             else:\n                pred_real = discriminator(real_b, real_a)\n                pred_fake = discriminator(fake_b.detach(), real_a)\n                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n\n        if scaler: scaler.scale(loss_D).backward(); scaler.step(optimizer_D); scaler.update()\n        else: loss_D.backward(); optimizer_D.step()\n\n        if i % 100 == 0: print(f\"E{epoch} B{i} L_D:{loss_D.item():.3f} L_G:{loss_G.item():.3f}\")\n\n    scheduler_G.step(); scheduler_D.step()\n    if (epoch+1) % 10 == 0: save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D)\n    if (epoch+1) % 5 == 0:\n        generator.eval()\n        with torch.no_grad():\n            f = generator(real_a[:3])\n            plt.figure(figsize=(9,9))\n            for k in range(3):\n                plt.subplot(3,3,k*3+1); plt.imshow(real_a[k].cpu().squeeze(),cmap='magma',origin='lower'); plt.axis('off')\n                plt.subplot(3,3,k*3+2); plt.imshow(real_b[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n                plt.subplot(3,3,k*3+3); plt.imshow(f[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n            plt.show()\n        generator.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:49:20.373615Z","iopub.execute_input":"2026-01-19T10:49:20.373918Z","iopub.status.idle":"2026-01-19T10:49:24.984412Z","shell.execute_reply.started":"2026-01-19T10:49:20.373897Z","shell.execute_reply":"2026-01-19T10:49:24.982844Z"}},"outputs":[{"name":"stdout","text":"Loading custom style texture from /kaggle/input/stylee/smooth-stucco-wall.jpg\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2208679721.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mreal_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"## 6. Kinetic 3D Experience (Synthetic Test)","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchaudio\nimport torchaudio.transforms as T\nimport cv2\nimport os\nfrom moviepy.editor import ImageSequenceClip, AudioFileClip\n\n# --- 1. CHARGEMENT DU CHECKPOINT AVEC GESTION DES CL√âS ---\ncheckpoint_path = \"/kaggle/input/checkpoints/checkpoint_epoch_199.pth\"\n\nif os.path.exists(checkpoint_path):\n    # Chargement sur le bon device (GPU/CPU)\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    \n    try:\n        # Test de la structure d√©tect√©e dans vos logs (cl√© 'G')\n        if isinstance(checkpoint, dict) and 'G' in checkpoint:\n            generator.load_state_dict(checkpoint['G'])\n            print(f\"‚úÖ Succ√®s : Poids charg√©s depuis la cl√© ['G'] (√âpoque {checkpoint.get('epoch', '199')})\")\n        elif isinstance(checkpoint, dict) and 'generator_state_dict' in checkpoint:\n            generator.load_state_dict(checkpoint['generator_state_dict'])\n            print(f\"‚úÖ Succ√®s : Poids charg√©s depuis ['generator_state_dict']\")\n        else:\n            # Tentative de chargement direct\n            generator.load_state_dict(checkpoint)\n            print(f\"‚úÖ Succ√®s : Poids charg√©s (Format direct)\")\n    except RuntimeError as e:\n        print(f\"‚ùå Erreur de correspondance d'architecture : {e}\")\n    \n    generator.eval()\nelse:\n    print(f\"‚ùå Fichier introuvable √† l'emplacement : {checkpoint_path}\")\n\n# --- 2. FONCTION DE G√âN√âRATION VID√âO KIN√âTIQUE (CORRIG√âE) ---\ndef generate_kinetic_video_fixed(audio_path, output_path=\"final_synesthesia_demo.mp4\", fps=30):\n    print(f\"üöÄ Lancement du rendu cin√©matographique pour : {audio_path}...\")\n    \n    # Chargement et normalisation audio\n    waveform, sr = torchaudio.load(audio_path)\n    if sr != SAMPLE_RATE: \n        waveform = T.Resample(sr, SAMPLE_RATE)(waveform)\n    if waveform.shape[0] > 1: # Mixage en mono si st√©r√©o\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n    window_samples = int(2.0 * SAMPLE_RATE)\n    step_samples = int(SAMPLE_RATE / fps)\n    temp_frames = []\n    \n    # Limitation pour la d√©mo (ex: 60 secondes)\n    max_samples = min(waveform.shape[1], 60 * SAMPLE_RATE)\n\n    with torch.no_grad():\n        for start in range(0, max_samples - window_samples, step_samples):\n            chunk = waveform[:, start:start+window_samples]\n            \n            # Utilisation de votre objet normalizer (doit √™tre d√©fini dans votre notebook)\n            spec = normalizer.transform(chunk) \n            fake_tensor = generator(spec.unsqueeze(0).to(DEVICE))\n            \n            # Post-processing image (Tanh -> [0, 1])\n            img_np = np.clip((fake_tensor.squeeze().permute(1,2,0).cpu().numpy() * 0.5 + 0.5), 0, 1)\n            \n            # Pr√©paration des donn√©es 3D\n            h_map = cv2.resize(img_np, (64, 64))\n            Z = np.dot(h_map[..., :3], [0.299, 0.587, 0.114]) # Luminance pour relief\n            X, Y = np.meshgrid(np.linspace(0, 1, 64), np.linspace(0, 1, 64))\n            \n            # Cr√©ation de la figure\n            fig = plt.figure(figsize=(12, 6), dpi=100)\n            \n            # Subplot 2D : L'Art G√©n√©ratif\n            ax2d = fig.add_subplot(1, 2, 1)\n            ax2d.imshow(img_np)\n            ax2d.axis('off')\n            ax2d.set_title(\"Art Synesth√©sique (IA)\", color='white', pad=10)\n            \n            # Subplot 3D : Topographie du son\n            ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n            ax3d.plot_surface(X, Y, Z, facecolors=h_map, linewidth=0, antialiased=True, shade=True)\n            ax3d.set_zlim(0, 1)\n            ax3d.view_init(elev=30, azim=start/500) # Rotation en fonction du temps\n            ax3d.axis('off')\n            ax3d.set_title(\"Topographie du Timbre\", color='white', pad=10)\n            \n            # Style sombre pour le rendu\n            fig.patch.set_facecolor('black')\n            for ax in fig.axes: ax.set_facecolor('black')\n\n            # --- FIX ATTRIBUTE ERROR : Utilisation de buffer_rgba ---\n            fig.canvas.draw()\n            rgba_buffer = fig.canvas.buffer_rgba()\n            frame = np.array(rgba_buffer)[:, :, :3] # Extraction RGB\n            temp_frames.append(frame)\n            plt.close(fig)\n            \n            if len(temp_frames) % 50 == 0:\n                print(f\"üé• √âtat d'avancement : {len(temp_frames)} frames g√©n√©r√©es...\")\n\n    if temp_frames:\n        # Assemblage final\n        clip = ImageSequenceClip(temp_frames, fps=fps)\n        audio_clip = AudioFileClip(audio_path).subclip(0, len(temp_frames)/fps)\n        final_video = clip.set_audio(audio_clip)\n        final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", bitrate=\"5000k\")\n        print(f\"‚ú® Vid√©o finale g√©n√©r√©e avec succ√®s : {output_path}\")\n\n# --- 3. EX√âCUTION DE LA D√âMO ---\nbeethoven_path = \"/kaggle/input/iggydd/jojo-stardust-crusaders-ost-iggy-death-theme_V13AwkU6.wav\"\n\nif os.path.exists(beethoven_path):\n    generate_kinetic_video_fixed(beethoven_path, \"beethoven_final_art.mp4\")\nelse:\n    print(\"‚ö†Ô∏è Fichier Beethoven absent. G√©n√©ration d'un fichier de test synth√©tique...\")\n    # S'assurer que kinetics_test.wav a √©t√© g√©n√©r√© dans la cellule pr√©c√©dente\n    if os.path.exists(\"kinetics_test.wav\"):\n        generate_kinetic_video_fixed(\"kinetics_test.wav\", \"test_art.mp4\")\n    else:\n        print(\"‚ùå Aucun fichier audio disponible pour le test.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:55:18.242660Z","iopub.execute_input":"2026-01-19T10:55:18.243049Z","iopub.status.idle":"2026-01-19T10:57:22.714687Z","shell.execute_reply.started":"2026-01-19T10:55:18.243022Z","shell.execute_reply":"2026-01-19T10:57:22.714021Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Succ√®s : Poids charg√©s depuis la cl√© ['G'] (√âpoque 199)\nüöÄ Lancement du rendu cin√©matographique pour : /kaggle/input/iggydd/jojo-stardust-crusaders-ost-iggy-death-theme_V13AwkU6.wav...\nüé• √âtat d'avancement : 50 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 100 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 150 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 200 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 250 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 300 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 350 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 400 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 450 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 500 frames g√©n√©r√©es...\nüé• √âtat d'avancement : 550 frames g√©n√©r√©es...\nMoviepy - Building video beethoven_final_art.mp4.\nMoviePy - Writing audio in beethoven_final_artTEMP_MPY_wvf_snd.mp4\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nMoviepy - Writing video beethoven_final_art.mp4\n\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Moviepy - Done !\nMoviepy - video ready beethoven_final_art.mp4\n‚ú® Vid√©o finale g√©n√©r√©e avec succ√®s : beethoven_final_art.mp4\n","output_type":"stream"}],"execution_count":11}]}