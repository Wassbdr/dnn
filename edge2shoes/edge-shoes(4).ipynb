{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683},{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":14537342,"sourceType":"datasetVersion","datasetId":9284938},{"sourceId":14537360,"sourceType":"datasetVersion","datasetId":9284950},{"sourceId":14541883,"sourceType":"datasetVersion","datasetId":9287947},{"sourceId":14541886,"sourceType":"datasetVersion","datasetId":9287949},{"sourceId":4508855,"sourceType":"kernelVersion"},{"sourceId":12703386,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pix2Pix - Image-to-Image Translation (v5 Anti-Artifacts)\n\nCe notebook implémente la version **v5** optimisée pour éliminer les artefacts de damier (checkerboard) et stabiliser l'entraînement.\n\n## Nouveautés v5 (Clean & Stable) :\n1.  **Resize-Convolution** : Remplacement de `ConvTranspose2d` par `Upsample` + `Conv2d` dans le générateur. Cela élimine les artefacts de damier.\n2.  **U-Net Architecture** : Retour au U-Net (vs ResNet) pour une meilleure transmission des détails spatiaux via les skip connections.\n3.  **Spectral Normalization** : Appliqué à **tout** le discriminateur pour éviter qu'il ne domine trop vite.\n4.  **Best Model Only** : Sauvegarde uniquement si la `Val L1 Loss` s'améliore.\n5.  **Hyperparamètres** : `L1_LAMBDA = 100`, `LR_G = 1e-4`, `LR_D = 4e-4`.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:33.899805Z","iopub.execute_input":"2026-01-18T23:42:33.900444Z","iopub.status.idle":"2026-01-18T23:42:33.905117Z","shell.execute_reply.started":"2026-01-18T23:42:33.900414Z","shell.execute_reply":"2026-01-18T23:42:33.904280Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 1. Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # TTUR: Deux Learning Rates differents (D > G)\n    LR_G = 1e-4\n    LR_D = 4e-4\n    BATCH_SIZE = 1 \n    NUM_WORKERS = 4 \n    IMAGE_SIZE = 256\n    CHANNELS_IMG = 3\n    L1_LAMBDA = 100 # Priorite absolue a la geometrie\n    NUM_EPOCHS = 100\n    LOAD_MODEL = False\n    SAVE_MODEL = True\n    CHECKPOINT_DISC = \"disc_v5.pth.tar\"\n    CHECKPOINT_GEN = \"gen_v5.pth.tar\"\n    \n    TRAIN_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/train\"\n    VAL_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/val\"\n    \n    TRAIN_SIZE_LIMIT = None \n    \nprint(f\"Device used: {Config.DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:33.906523Z","iopub.execute_input":"2026-01-18T23:42:33.906744Z","iopub.status.idle":"2026-01-18T23:42:33.921008Z","shell.execute_reply.started":"2026-01-18T23:42:33.906725Z","shell.execute_reply":"2026-01-18T23:42:33.920460Z"}},"outputs":[{"name":"stdout","text":"Device used: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 2. Pipeline de Données","metadata":{}},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    def __init__(self, root_dir):\n        self.root_dir = root_dir\n        if os.path.exists(root_dir):\n            self.list_files = os.listdir(root_dir)\n        else:\n            print(f\"Attention: Le dossier {root_dir} n'existe pas.\")\n            self.list_files = []\n\n    def __len__(self):\n        return len(self.list_files)\n\n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        try:\n            image = np.array(Image.open(img_path))\n        except:\n            return self.__getitem__(index + 1)\n\n        w = image.shape[1]\n        cutoff = w // 2\n        input_image = image[:, :cutoff, :] \n        target_image = image[:, cutoff:, :]\n\n        input_image = Image.fromarray(input_image)\n        target_image = Image.fromarray(target_image)\n        \n        # Resize + RandomCrop\n        resize = transforms.Resize((286, 286))\n        input_image = resize(input_image)\n        target_image = resize(target_image)\n        \n        i, j, h, w_crop = transforms.RandomCrop.get_params(\n            input_image, output_size=(Config.IMAGE_SIZE, Config.IMAGE_SIZE)\n        )\n        input_image = transforms.functional.crop(input_image, i, j, h, w_crop)\n        target_image = transforms.functional.crop(target_image, i, j, h, w_crop)\n        \n        # Random Flip\n        if torch.rand(1) > 0.5:\n            input_image = transforms.functional.hflip(input_image)\n            target_image = transforms.functional.hflip(target_image)\n            \n        base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n        ])\n        \n        input_image = base_transform(input_image)\n        target_image = base_transform(target_image)\n\n        return input_image, target_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:33.921764Z","iopub.execute_input":"2026-01-18T23:42:33.921940Z","iopub.status.idle":"2026-01-18T23:42:33.936674Z","shell.execute_reply.started":"2026-01-18T23:42:33.921923Z","shell.execute_reply":"2026-01-18T23:42:33.936003Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## 3. Architecture U-Net (Resize-Convolution)","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n        super().__init__()\n        if down:\n            self.conv = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\"),\n                nn.InstanceNorm2d(out_channels, affine=True),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            # RESIZE-CONVOLUTION (Anti-Checkerboard)\n            self.conv = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n                nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode=\"reflect\"),\n                nn.InstanceNorm2d(out_channels, affine=True),\n                nn.ReLU(inplace=True),\n            )\n\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.down = down\n\n    def forward(self, x):\n        x = self.conv(x)\n        return self.dropout(x) if self.use_dropout else x\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, features=64):\n        super().__init__()\n        # Initial Down\n        self.initial_down = nn.Sequential(\n            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        \n        # Encoder path\n        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n        self.down2 = Block(features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False)\n        self.down3 = Block(features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down4 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down5 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down6 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        \n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(features * 8, features * 8, 4, 2, 1, padding_mode=\"reflect\"),\n            nn.ReLU(inplace=True),\n        )\n\n        # Decoder path (Upsample + Conv)\n        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up2 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up3 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up4 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False)\n        self.up5 = Block(features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False)\n        self.up6 = Block(features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False)\n        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n        \n        # Final output layer (Resize first then Conv)\n        self.final_up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(features * 2, in_channels, kernel_size=3, stride=1, padding=1, padding_mode=\"reflect\"),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        d1 = self.initial_down(x)\n        d2 = self.down1(d1)\n        d3 = self.down2(d2)\n        d4 = self.down3(d3)\n        d5 = self.down4(d4)\n        d6 = self.down5(d5)\n        d7 = self.down6(d6)\n        bottleneck = self.bottleneck(d7)\n        \n        # Skip connections concat\n        u1 = self.up1(bottleneck)\n        u2 = self.up2(torch.cat([u1, d7], 1))\n        u3 = self.up3(torch.cat([u2, d6], 1))\n        u4 = self.up4(torch.cat([u3, d5], 1))\n        u5 = self.up5(torch.cat([u4, d4], 1))\n        u6 = self.up6(torch.cat([u5, d3], 1))\n        u7 = self.up7(torch.cat([u6, d2], 1))\n        \n        return self.final_up(torch.cat([u7, d1], 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:33.938001Z","iopub.execute_input":"2026-01-18T23:42:33.938210Z","iopub.status.idle":"2026-01-18T23:42:33.957222Z","shell.execute_reply.started":"2026-01-18T23:42:33.938192Z","shell.execute_reply":"2026-01-18T23:42:33.956637Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## 4. Architecture Discriminator (Full Spectral Norm)","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        # Initial layer: Spectral Norm\n        self.initial = nn.Sequential(\n            nn.utils.spectral_norm(\n                nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")\n            ),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            # All Conv layers: Spectral Norm\n            layers.append(\n                nn.Sequential(\n                    nn.utils.spectral_norm(\n                        nn.Conv2d(in_channels, feature, 4, stride=1 if feature == features[-1] else 2, padding=1, bias=False, padding_mode=\"reflect\")\n                    ),\n                    nn.InstanceNorm2d(feature, affine=True),\n                    nn.LeakyReLU(0.2, inplace=True),\n                )\n            )\n            in_channels = feature\n\n        # Final layer: Spectral Norm (Critical for stability)\n        layers.append(\n            nn.utils.spectral_norm(\n                nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n            )\n        )\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        x = self.initial(x)\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:33.957873Z","iopub.execute_input":"2026-01-18T23:42:33.958133Z","iopub.status.idle":"2026-01-18T23:42:33.980235Z","shell.execute_reply.started":"2026-01-18T23:42:33.958102Z","shell.execute_reply":"2026-01-18T23:42:33.979530Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## 5. Training Loop v5","metadata":{}},{"cell_type":"code","source":"def weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        nn.init.kaiming_normal_(m.weight, a=0.02, mode='fan_in', nonlinearity='leaky_relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight, 1.0, 0.02)\n        nn.init.constant_(m.bias, 0)\n\nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return torch.cat(to_return)\n\ndef train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, mse_loss, g_scaler, d_scaler, replay_buffer):\n    loop = tqdm(loader, leave=True)\n\n    for idx, (x, y) in enumerate(loop):\n        x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n        \n        # --- Generator Forward ---\n        with torch.amp.autocast('cuda'):\n            y_fake = gen(x)\n            D_fake = disc(x, y_fake)\n            # LSGAN: Target 1.0\n            G_gan_loss = mse_loss(D_fake, torch.ones_like(D_fake))\n            \n            G_l1_loss = l1_loss(y_fake, y) * Config.L1_LAMBDA\n            G_loss = G_gan_loss + G_l1_loss\n\n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # --- Discriminator Forward ---\n        with torch.amp.autocast('cuda'):\n            y_fake_buffer = replay_buffer.push_and_pop(y_fake.detach())\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake_buffer)\n            \n            D_real_loss = mse_loss(D_real, torch.ones_like(D_real))\n            D_fake_loss = mse_loss(D_fake, torch.zeros_like(D_fake))\n            \n            D_loss = (D_real_loss + D_fake_loss) * 0.5\n\n        opt_disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n\n        if idx % 10 == 0:\n            loop.set_postfix(D=D_loss.item(), G=G_loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:34.006148Z","iopub.execute_input":"2026-01-18T23:42:34.006339Z","iopub.status.idle":"2026-01-18T23:42:34.016583Z","shell.execute_reply.started":"2026-01-18T23:42:34.006321Z","shell.execute_reply":"2026-01-18T23:42:34.015975Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## 6. Main avec Validation & Best Model Logic","metadata":{}},{"cell_type":"code","source":"def validate(gen, val_loader, l1_loss):\n    gen.eval()\n    total_l1 = 0\n    count = 0\n    with torch.no_grad():\n        # On peut limiter la validation a qques batchs pour la rapidité\n        for x, y in val_loader:\n            x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n            y_fake = gen(x)\n            total_l1 += l1_loss(y_fake, y).item()\n            count += 1\n    gen.train()\n    return total_l1 / count if count > 0 else float('inf')\n\ndef save_some_examples(gen, val_loader, epoch, folder):\n    try:\n        x, y = next(iter(val_loader))\n    except:\n        return\n    x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5\n        x = x * 0.5 + 0.5\n        y = y * 0.5 + 0.5\n        save_image(y_fake, folder + f\"/gen_{epoch}.png\")\n        save_image(x, folder + f\"/input_{epoch}.png\")\n        save_image(y, folder + f\"/label_{epoch}.png\")\n    gen.train()\n\ndef main():\n    disc = Discriminator().to(Config.DEVICE)\n    gen = Generator(features=64).to(Config.DEVICE)\n    \n    disc.apply(weights_init)\n    gen.apply(weights_init)\n    \n    # TTUR: D plus elevé\n    opt_disc = optim.Adam(disc.parameters(), lr=Config.LR_D, betas=(0.5, 0.999))\n    opt_gen = optim.Adam(gen.parameters(), lr=Config.LR_G, betas=(0.5, 0.999))\n\n    MSE_LOSS = nn.MSELoss()\n    L1_LOSS = nn.L1Loss()\n\n    train_dataset = Pix2PixDataset(root_dir=Config.TRAIN_DIR)\n    \n    if Config.TRAIN_SIZE_LIMIT and len(train_dataset) > Config.TRAIN_SIZE_LIMIT:\n        indices = torch.randperm(len(train_dataset))[:Config.TRAIN_SIZE_LIMIT]\n        train_dataset = torch.utils.data.Subset(train_dataset, indices)\n\n    if len(train_dataset) > 0:\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=True,\n        )\n        val_dataset = Pix2PixDataset(root_dir=Config.VAL_DIR)\n        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False) if len(val_dataset) > 0 else train_loader\n\n        g_scaler = torch.amp.GradScaler('cuda')\n        d_scaler = torch.amp.GradScaler('cuda')\n        replay_buffer = ReplayBuffer()\n        \n        best_val_loss = float('inf')\n\n        for epoch in range(Config.NUM_EPOCHS):\n            print(f\"Epoch {epoch}/{Config.NUM_EPOCHS} (Best L1: {best_val_loss:.4f})\")\n            train_fn(\n                disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, MSE_LOSS, g_scaler, d_scaler, replay_buffer\n            )\n            \n            # Validation logique\n            current_val_loss = validate(gen, val_loader, L1_LOSS)\n            print(f\"Val L1 Loss: {current_val_loss:.4f}\")\n            \n            if Config.SAVE_MODEL and current_val_loss < best_val_loss:\n                print(\"New Best Model! Saving...\")\n                best_val_loss = current_val_loss\n                torch.save(gen.state_dict(), Config.CHECKPOINT_GEN)\n                torch.save(disc.state_dict(), Config.CHECKPOINT_DISC)\n                \n            if epoch % 5 == 0:\n                save_some_examples(gen, val_loader, epoch, folder=\"evaluation\")\n\n    else:\n        print(\"No data found.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:42:34.017708Z","iopub.execute_input":"2026-01-18T23:42:34.017914Z"}},"outputs":[{"name":"stdout","text":"Epoch 0/100 (Best L1: inf)\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 554/49825 [00:31<42:41, 19.24it/s, D=0.301, G=10.1]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 7. Inference","metadata":{}},{"cell_type":"code","source":"def predict_custom_sketch(image_path):\n    gen = Generator(features=64).to(Config.DEVICE)\n    if os.path.exists(Config.CHECKPOINT_GEN):\n        gen.load_state_dict(torch.load(Config.CHECKPOINT_GEN, map_location=Config.DEVICE))\n        gen.eval()\n        \n        if not os.path.exists(image_path):\n            print(\"Image introuvable\")\n            return\n            \n        img = Image.open(image_path).convert(\"RGB\")\n        t = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n        x = t(img).unsqueeze(0).to(Config.DEVICE)\n        with torch.no_grad():\n            res = gen(x).squeeze().cpu() * 0.5 + 0.5\n            plt.imshow(res.permute(1, 2, 0))\n            plt.axis(\"off\")\n            plt.show()\n    else:\n        print(\"Erreur: Pas de checkpoint.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}