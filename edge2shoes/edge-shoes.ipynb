{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683},{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":14537342,"sourceType":"datasetVersion","datasetId":9284938},{"sourceId":14537360,"sourceType":"datasetVersion","datasetId":9284950},{"sourceId":4508855,"sourceType":"kernelVersion"},{"sourceId":12703386,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pix2Pix - Image-to-Image Translation in Raw PyTorch\n\nCe notebook implémente l'architecture Pix2Pix (Isola et al., 2017) pour la traduction d'images (ex: croquis -> photo, jour -> nuit) en utilisant **PyTorch pur**.\n\n## Objectifs Techniques\n- **Architecture**: Generator U-Net + Discriminator PatchGAN.\n- **Loss**: Adversarial Loss (BCE) + L1 Loss (Reconstruction).\n- **Dataset**: Gestion d'images paires concaténées horizontalement.\n- **Training**: Boucle d'entraînement personnalisée avec mises à jour alternées.\n\n## Mises à jour v2 (Performance):\n- **Instance Normalization**: Remplace BatchNorm (meilleur pour batch_size=1).\n- **Spectral Normalization**: Ajouté au Discriminateur pour la stabilité.\n- **LR Scheduler**: Décroissance du learning rate.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:57.963820Z","iopub.execute_input":"2026-01-18T14:31:57.964491Z","iopub.status.idle":"2026-01-18T14:31:57.968901Z","shell.execute_reply.started":"2026-01-18T14:31:57.964455Z","shell.execute_reply":"2026-01-18T14:31:57.968155Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"## 1. Configuration & Hyperparamètres","metadata":{}},{"cell_type":"code","source":"class Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    LEARNING_RATE = 2e-4\n    BATCH_SIZE = 1 \n    NUM_WORKERS = 2\n    IMAGE_SIZE = 256\n    CHANNELS_IMG = 3\n    L1_LAMBDA = 100 \n    NUM_EPOCHS = 50 # Edges2Shoes converge plus lentement, mais on commence avec 50\n    LOAD_MODEL = False\n    SAVE_MODEL = True\n    CHECKPOINT_DISC = \"disc.pth.tar\"\n    CHECKPOINT_GEN = \"gen.pth.tar\"\n    \n    # --- CONFIGURATION DATASET ---\n    # Edges2Shoes (Active par defaut maintenant)\n    TRAIN_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/train\"\n    VAL_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/val\"\n    \n    # Facades (Commenté)\n    # TRAIN_DIR = \"/kaggle/input/pix2pix-dataset/facades/facades/train\"\n    # VAL_DIR = \"/kaggle/input/pix2pix-dataset/facades/facades/val\"\n    \n    # Limite pour debug (None = tout le dataset)\n    TRAIN_SIZE_LIMIT = None \n    \nprint(f\"Device used: {Config.DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:57.970168Z","iopub.execute_input":"2026-01-18T14:31:57.970389Z","iopub.status.idle":"2026-01-18T14:31:57.985716Z","shell.execute_reply.started":"2026-01-18T14:31:57.970369Z","shell.execute_reply":"2026-01-18T14:31:57.985154Z"}},"outputs":[{"name":"stdout","text":"Device used: cuda\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"## 2. Pipeline de Données (Paired Dataset)","metadata":{}},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    def __init__(self, root_dir):\n        self.root_dir = root_dir\n        if os.path.exists(root_dir):\n            self.list_files = os.listdir(root_dir)\n        else:\n            print(f\"Attention: Le dossier {root_dir} n'existe pas. Assurez-vous de charger vos donnees.\")\n            self.list_files = []\n\n    def __len__(self):\n        return len(self.list_files)\n\n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        try:\n            image = np.array(Image.open(img_path))\n        except:\n            return self.__getitem__(index + 1) # Skip corrupted images\n\n        w = image.shape[1]\n        cutoff = w // 2\n        input_image = image[:, :cutoff, :] \n        target_image = image[:, cutoff:, :]\n\n        aug_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((286, 286)),\n            transforms.RandomCrop(Config.IMAGE_SIZE),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n        # Augmentation manuelle pour garantir la meme transformation\n        input_image = Image.fromarray(input_image)\n        target_image = Image.fromarray(target_image)\n        \n        resize = transforms.Resize((286, 286))\n        input_image = resize(input_image)\n        target_image = resize(target_image)\n        \n        i, j, h, w_crop = transforms.RandomCrop.get_params(\n            input_image, output_size=(Config.IMAGE_SIZE, Config.IMAGE_SIZE)\n        )\n        input_image = transforms.functional.crop(input_image, i, j, h, w_crop)\n        target_image = transforms.functional.crop(target_image, i, j, h, w_crop)\n        \n        if torch.rand(1) > 0.5:\n            input_image = transforms.functional.hflip(input_image)\n            target_image = transforms.functional.hflip(target_image)\n            \n        base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n        ])\n        \n        input_image = base_transform(input_image)\n        target_image = base_transform(target_image)\n\n        return input_image, target_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:57.986899Z","iopub.execute_input":"2026-01-18T14:31:57.987169Z","iopub.status.idle":"2026-01-18T14:31:57.999715Z","shell.execute_reply.started":"2026-01-18T14:31:57.987148Z","shell.execute_reply":"2026-01-18T14:31:57.998952Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"## 3. Architecture du Discriminateur (PatchGAN) - Upgrade: SpectralNorm & InstanceNorm","metadata":{}},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n            # Spectral Norm stabilise l'entrainement du discriminateur\n            nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\")),\n            nn.InstanceNorm2d(out_channels, affine=True), # InstanceNorm est preferable pour Pix2Pix\n            nn.LeakyReLU(0.2),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\")),\n            nn.LeakyReLU(0.2),\n        )\n\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            layers.append(\n                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2)\n            )\n            in_channels = feature\n\n        layers.append(\n            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n        )\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        x = self.initial(x)\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:58.001113Z","iopub.execute_input":"2026-01-18T14:31:58.001359Z","iopub.status.idle":"2026-01-18T14:31:58.016087Z","shell.execute_reply.started":"2026-01-18T14:31:58.001338Z","shell.execute_reply":"2026-01-18T14:31:58.015316Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"## 4. Architecture du Générateur (U-Net) - Upgrade: InstanceNorm","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n            if down\n            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            # InstanceNorm remplace BatchNorm\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n        )\n\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.down = down\n\n    def forward(self, x):\n        x = self.block(x)\n        return self.dropout(x) if self.use_dropout else x\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, features=64):\n        super().__init__()\n        self.initial_down = nn.Sequential(\n            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n            nn.LeakyReLU(0.2),\n        )\n        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n        self.down2 = Block(features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False)\n        self.down3 = Block(features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down4 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down5 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        self.down6 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n        \n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(features * 8, features * 8, 4, 2, 1, padding_mode=\"reflect\"),\n            nn.ReLU(),\n        )\n\n        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up2 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up3 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n        self.up4 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False)\n        self.up5 = Block(features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False)\n        self.up6 = Block(features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False)\n        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n        \n        self.final_up = nn.Sequential(\n            nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        d1 = self.initial_down(x)\n        d2 = self.down1(d1)\n        d3 = self.down2(d2)\n        d4 = self.down3(d3)\n        d5 = self.down4(d4)\n        d6 = self.down5(d5)\n        d7 = self.down6(d6)\n        bottleneck = self.bottleneck(d7)\n        \n        u1 = self.up1(bottleneck)\n        u2 = self.up2(torch.cat([u1, d7], 1))\n        u3 = self.up3(torch.cat([u2, d6], 1))\n        u4 = self.up4(torch.cat([u3, d5], 1))\n        u5 = self.up5(torch.cat([u4, d4], 1))\n        u6 = self.up6(torch.cat([u5, d3], 1))\n        u7 = self.up7(torch.cat([u6, d2], 1))\n        \n        return self.final_up(torch.cat([u7, d1], 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:58.063619Z","iopub.execute_input":"2026-01-18T14:31:58.063907Z","iopub.status.idle":"2026-01-18T14:31:58.076874Z","shell.execute_reply.started":"2026-01-18T14:31:58.063884Z","shell.execute_reply":"2026-01-18T14:31:58.076238Z"}},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"## 5. Fonctions Utilitaires & Visualisation","metadata":{}},{"cell_type":"code","source":"def save_some_examples(gen, val_loader, epoch, folder):\n    try:\n        x, y = next(iter(val_loader))\n    except StopIteration:\n        return\n    x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n        \n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5\n        x = x * 0.5 + 0.5\n        y = y * 0.5 + 0.5\n        save_image(y_fake, folder + f\"/gen_{epoch}.png\")\n        save_image(x, folder + f\"/input_{epoch}.png\")\n        save_image(y, folder + f\"/label_{epoch}.png\")\n        \n    gen.train()\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:58.078073Z","iopub.execute_input":"2026-01-18T14:31:58.078288Z","iopub.status.idle":"2026-01-18T14:31:58.092286Z","shell.execute_reply.started":"2026-01-18T14:31:58.078268Z","shell.execute_reply":"2026-01-18T14:31:58.091609Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"## 6. Boucle d'Entraînement (Update: Scheduler)","metadata":{}},{"cell_type":"code","source":"def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler):\n    loop = tqdm(loader, leave=True)\n\n    for idx, (x, y) in enumerate(loop):\n        x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n        \n        with torch.amp.autocast('cuda'):\n            y_fake = gen(x)\n            D_real = disc(x, y)\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            \n            D_fake = disc(x, y_fake.detach())\n            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n            \n            D_loss = (D_real_loss + D_fake_loss) / 2\n\n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n\n        with torch.amp.autocast('cuda'):\n            D_fake = disc(x, y_fake)\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n            L1 = l1_loss(y_fake, y) * Config.L1_LAMBDA\n            G_loss = G_fake_loss + L1\n\n        gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n\n        if idx % 10 == 0:\n            loop.set_postfix(\n                D_loss=D_loss.item(),\n                G_loss=G_loss.item(),\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:58.093018Z","iopub.execute_input":"2026-01-18T14:31:58.093888Z","iopub.status.idle":"2026-01-18T14:31:58.107982Z","shell.execute_reply.started":"2026-01-18T14:31:58.093865Z","shell.execute_reply":"2026-01-18T14:31:58.107295Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"## 7. Execution Principale","metadata":{}},{"cell_type":"code","source":"def main():\n    disc = Discriminator(in_channels=3).to(Config.DEVICE)\n    gen = Generator(in_channels=3, features=64).to(Config.DEVICE)\n    \n    disc.apply(weights_init)\n    gen.apply(weights_init)\n    \n    opt_disc = optim.Adam(disc.parameters(), lr=Config.LEARNING_RATE, betas=(0.5, 0.999))\n    opt_gen = optim.Adam(gen.parameters(), lr=Config.LEARNING_RATE, betas=(0.5, 0.999))\n\n    # Scheduler pour decroitre le LR apres 50% de l'entrainement\n    # StepLR qui multiplie par 0.1 tous les 30 epochs (exemple simple)\n    scheduler_gen = optim.lr_scheduler.StepLR(opt_gen, step_size=30, gamma=0.1)\n    scheduler_disc = optim.lr_scheduler.StepLR(opt_disc, step_size=30, gamma=0.1)\n\n    BCE = nn.BCEWithLogitsLoss()\n    L1_LOSS = nn.L1Loss()\n\n    train_dataset = Pix2PixDataset(root_dir=Config.TRAIN_DIR)\n\n    if Config.TRAIN_SIZE_LIMIT is not None and len(train_dataset) > Config.TRAIN_SIZE_LIMIT:\n        print(f\"Limitation du dataset train a {Config.TRAIN_SIZE_LIMIT} images.\")\n        indices = torch.randperm(len(train_dataset))[:Config.TRAIN_SIZE_LIMIT]\n        train_dataset = torch.utils.data.Subset(train_dataset, indices)\n\n    if len(train_dataset) > 0:\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=True,\n        )\n        val_dataset = Pix2PixDataset(root_dir=Config.VAL_DIR)\n        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False) if len(val_dataset) > 0 else train_loader\n        \n        g_scaler = torch.amp.GradScaler('cuda')\n        d_scaler = torch.amp.GradScaler('cuda')\n\n        for epoch in range(Config.NUM_EPOCHS):\n            print(f\"Epoch [{epoch}/{Config.NUM_EPOCHS}] - LR: {scheduler_gen.get_last_lr()[0]}\")\n            train_fn(\n                disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler\n            )\n            \n            # Step du scheduler\n            scheduler_gen.step()\n            scheduler_disc.step()\n\n            if Config.SAVE_MODEL and epoch % 5 == 0:\n                print(\"Sauvegarde du modele...\")\n                torch.save(gen.state_dict(), Config.CHECKPOINT_GEN)\n                torch.save(disc.state_dict(), Config.CHECKPOINT_DISC)\n\n            save_some_examples(gen, val_loader, epoch, folder=\"evaluation\")\n    else:\n        print(f\"Aucune image trouvee dans {Config.TRAIN_DIR}.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:31:58.109454Z","iopub.execute_input":"2026-01-18T14:31:58.109863Z"}},"outputs":[{"name":"stdout","text":"Epoch [0/50] - LR: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 1324/49825 [01:10<41:38, 19.41it/s, D_loss=0.172, G_loss=49.1]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 8. Inférence & Démo","metadata":{}},{"cell_type":"code","source":"def run_inference():\n    print(\"--- Lancement de l'Inference ---\")\n    gen = Generator(in_channels=3, features=64).to(Config.DEVICE)\n    \n    if os.path.exists(Config.CHECKPOINT_GEN):\n        checkpoint = torch.load(Config.CHECKPOINT_GEN, map_location=Config.DEVICE)\n        gen.load_state_dict(checkpoint)\n        gen.eval()\n    else:\n        print(\"Pas de checkpoint trouve.\")\n        return\n\n    val_dataset = Pix2PixDataset(root_dir=Config.VAL_DIR)\n    if len(val_dataset) == 0: \n        print(\"Pas de dataset validation disponible\")\n        return\n        \n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n    n_samples = 5\n    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5*n_samples))\n    \n    with torch.no_grad():\n        for i, (x, y) in enumerate(val_loader):\n            if i >= n_samples: break\n            x = x.to(Config.DEVICE)\n            y_fake = gen(x).cpu()\n            x = x.cpu()\n            \n            x = x * 0.5 + 0.5\n            y = y * 0.5 + 0.5\n            y_fake = y_fake * 0.5 + 0.5\n            \n            axes[i, 0].imshow(x.squeeze().permute(1, 2, 0))\n            axes[i, 0].axis(\"off\")\n            \n            axes[i, 1].imshow(y_fake.squeeze().permute(1, 2, 0))\n            axes[i, 1].axis(\"off\")\n\n            axes[i, 2].imshow(y.squeeze().permute(1, 2, 0))\n            axes[i, 2].axis(\"off\")\n    plt.show()\n\ndef predict_custom_sketch(image_path):\n    print(f\"--- Prediction sur {image_path} ---\")\n    gen = Generator(in_channels=3, features=64).to(Config.DEVICE)\n    if os.path.exists(Config.CHECKPOINT_GEN):\n        gen.load_state_dict(torch.load(Config.CHECKPOINT_GEN, map_location=Config.DEVICE))\n        gen.eval()\n    else:\n        print(\"Erreur: Pas de checkpoint.\")\n        return\n\n    if not os.path.exists(image_path):\n        print(\"Image introuvable\")\n        return\n        \n    img = Image.open(image_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    x = t(img).unsqueeze(0).to(Config.DEVICE)\n    with torch.no_grad():\n        res = gen(x).squeeze().cpu() * 0.5 + 0.5\n        plt.imshow(res.permute(1, 2, 0))\n        plt.show()\n\n# run_inference()\n# predict_custom_sketch(\"test.jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}