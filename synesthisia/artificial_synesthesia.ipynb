{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artificial Synesthesia: Audio-to-Visual Generation with Pix2Pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "IMG_SIZE = 256\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 1.0 \n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 256 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Synthetic Dataset Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SyntheticAudioGenerator:\n",
        "    def __init__(self, sample_rate=22050, duration=1.0):\n",
        "        self.sr = sample_rate\n",
        "        self.duration = duration\n",
        "        self.n_samples = int(sample_rate * duration)\n",
        "\n",
        "    def generate_sine(self, freq=None):\n",
        "        if freq is None:\n",
        "            freq = random.uniform(200, 1000)\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        audio = np.sin(2 * np.pi * freq * t)\n",
        "        return audio.astype(np.float32), \"sine\", freq\n",
        "\n",
        "    def generate_white_noise(self):\n",
        "        audio = np.random.normal(0, 0.5, self.n_samples)\n",
        "        return audio.astype(np.float32), \"noise\", 0\n",
        "\n",
        "    def generate_chirp(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        f0 = random.uniform(100, 400)\n",
        "        f1 = random.uniform(800, 1500)\n",
        "        k = (f1 - f0) / self.duration\n",
        "        audio = np.sin(2 * np.pi * (f0 * t + 0.5 * k * t**2))\n",
        "        return audio.astype(np.float32), \"chirp\", (f0, f1)\n",
        "\n",
        "class VisualArtGenerator:\n",
        "    def __init__(self, img_size=256):\n",
        "        self.size = img_size\n",
        "\n",
        "    def normalized_to_img(self, data):\n",
        "        if len(data.shape) == 2:\n",
        "            data = np.stack([data]*3, axis=-1)\n",
        "        return (data * 255).astype(np.uint8)\n",
        "\n",
        "    def generate_gradient(self, color_phase=0.0):\n",
        "        x = np.linspace(0, 1, self.size)\n",
        "        y = np.linspace(0, 1, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        R = Y\n",
        "        G = np.sin(Y * np.pi + color_phase) * 0.5 + 0.5\n",
        "        B = 1.0 - Y\n",
        "        img = np.stack([R, G, B], axis=-1)\n",
        "        return img.astype(np.float32)\n",
        "\n",
        "    def generate_noise_texture(self):\n",
        "        noise = np.random.uniform(0, 1, (self.size, self.size, 3))\n",
        "        noise = np.clip((noise - 0.5) * 2.0 + 0.5, 0, 1)\n",
        "        noise[..., 1] *= 0.5 \n",
        "        return noise.astype(np.float32)\n",
        "\n",
        "    def generate_structured_pattern(self):\n",
        "        x = np.linspace(0, 10, self.size)\n",
        "        y = np.linspace(0, 10, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = np.sin(X + Y) * 0.5 + 0.5\n",
        "        img = np.stack([Z, Z, 1-Z], axis=-1) \n",
        "        return img.astype(np.float32)\n",
        "\n",
        "class SynesthesiaDataset(Dataset):\n",
        "    def __init__(self, size=1000, img_size=256, sample_rate=22050):\n",
        "        self.size = size\n",
        "        self.audio_gen = SyntheticAudioGenerator(sample_rate)\n",
        "        self.vis_gen = VisualArtGenerator(img_size)\n",
        "        \n",
        "        self.mel_transform = T.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=N_FFT,\n",
        "            win_length=N_FFT,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_mels=img_size,\n",
        "            power=2.0\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        choice = random.choice([\"sine\", \"noise\", \"chirp\"])\n",
        "        if choice == \"sine\":\n",
        "            audio, type_, param = self.audio_gen.generate_sine()\n",
        "            target_img_np = self.vis_gen.generate_gradient(color_phase=random.random())\n",
        "        elif choice == \"noise\":\n",
        "            audio, type_, param = self.audio_gen.generate_white_noise()\n",
        "            target_img_np = self.vis_gen.generate_noise_texture()\n",
        "        else:\n",
        "            audio, type_, param = self.audio_gen.generate_chirp()\n",
        "            target_img_np = self.vis_gen.generate_structured_pattern()\n",
        "\n",
        "        audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n",
        "        spec = self.mel_transform(audio_tensor) \n",
        "        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
        "        spec = (spec + 40) / 40 \n",
        "        spec = torch.clamp(spec, -1, 1)\n",
        "        spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "        target_img = torch.from_numpy(target_img_np).permute(2, 0, 1)\n",
        "        target_img = (target_img * 2.0) - 1.0\n",
        "\n",
        "        return spec, target_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture (U-Net & PatchGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5) \n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) \n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "        return self.final(u7)\n",
        "\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels + 3, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)), \n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Hyperparameters\n",
        "LR = 0.0002\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "EPOCHS = 10 \n",
        "BATCH_SIZE = 16 \n",
        "LAMBDA_PIXEL = 100\n",
        "\n",
        "# Initialize\n",
        "generator = UNetGenerator().to(DEVICE)\n",
        "discriminator = PatchGANDiscriminator().to(DEVICE)\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "criterion_GAN = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "criterion_pixelwise = nn.L1Loss().to(DEVICE)\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(B1, B2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(B1, B2))\n",
        "\n",
        "# Dataset\n",
        "dataloader = DataLoader(\n",
        "    SynesthesiaDataset(size=2000), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "else:\n",
        "    scaler = None \n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (spec, target_img) in enumerate(dataloader):\n",
        "        \n",
        "        real_a = spec.to(DEVICE)       \n",
        "        real_b = target_img.to(DEVICE) \n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                fake_b = generator(real_a)\n",
        "                pred_fake = discriminator(fake_b, real_a) \n",
        "                valid = torch.ones_like(pred_fake)\n",
        "                loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "                loss_pixel = criterion_pixelwise(fake_b, real_b)\n",
        "                loss_G = loss_GAN + LAMBDA_PIXEL * loss_pixel\n",
        "            scaler.scale(loss_G).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            fake_b = generator(real_a)\n",
        "            pred_fake = discriminator(fake_b, real_a)\n",
        "            valid = torch.ones_like(pred_fake)\n",
        "            loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "            loss_pixel = criterion_pixelwise(fake_b, real_b)\n",
        "            loss_G = loss_GAN + LAMBDA_PIXEL * loss_pixel\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                pred_real = discriminator(real_b, real_a) \n",
        "                valid = torch.ones_like(pred_real) \n",
        "                loss_real = criterion_GAN(pred_real, valid)\n",
        "                pred_fake = discriminator(fake_b.detach(), real_a) \n",
        "                fake = torch.zeros_like(pred_fake)\n",
        "                loss_fake = criterion_GAN(pred_fake, fake)\n",
        "                loss_D = 0.5 * (loss_real + loss_fake)\n",
        "            scaler.scale(loss_D).backward()\n",
        "            scaler.step(optimizer_D)\n",
        "            scaler.update()\n",
        "        else:\n",
        "             pred_real = discriminator(real_b, real_a)\n",
        "             valid = torch.ones_like(pred_real)\n",
        "             loss_real = criterion_GAN(pred_real, valid)\n",
        "             pred_fake = discriminator(fake_b.detach(), real_a)\n",
        "             fake = torch.zeros_like(pred_fake)\n",
        "             loss_fake = criterion_GAN(pred_fake, fake)\n",
        "             loss_D = 0.5 * (loss_real + loss_fake)\n",
        "             loss_D.backward()\n",
        "             optimizer_D.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(dataloader)}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Video Inference Pipeline (New)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_synesthesia_video(audio_path, output_path=\"synesthesia.mp4\", fps=30):\n",
        "    print(f\"Processing video for: {audio_path}\")\n",
        "    \n",
        "    # Load audio\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if sr != SAMPLE_RATE:\n",
        "        resampler = T.Resample(sr, SAMPLE_RATE)\n",
        "        waveform = resampler(waveform)\n",
        "    \n",
        "    # Define window logic (2-second windows as requested, overlapping)\n",
        "    # To get smooth video at FPS, we slide the window by step_size\n",
        "    window_duration = 2.0 \n",
        "    window_size = int(window_duration * SAMPLE_RATE)\n",
        "    step_size = int(SAMPLE_RATE / fps)\n",
        "    \n",
        "    # MelTransform reusable (needs to handle arbitrary length or fixed? \n",
        "    # We will slice waveform first then transform)\n",
        "    mel_transform = T.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT,\n",
        "        win_length=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=IMG_SIZE,\n",
        "        power=2.0\n",
        "    )\n",
        "    \n",
        "    frames = []\n",
        "    generator.eval()\n",
        "    \n",
        "    # Length check\n",
        "    total_samples = waveform.shape[1]\n",
        "    \n",
        "    print(\"Generating frames...\")\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, total_samples - window_size, step_size):\n",
        "            end = start + window_size\n",
        "            chunk = waveform[:, start:end]\n",
        "            \n",
        "            # Spectrogram\n",
        "            spec = mel_transform(chunk)\n",
        "            spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
        "            spec = (spec + 40) / 40\n",
        "            spec = torch.clamp(spec, -1, 1)\n",
        "            \n",
        "            # Resize logic: The request asked for 2-second windows.\n",
        "            # Our model was trained on 1-second spectrograms (approx 86 frames width).\n",
        "            # A 2-second spectrogram will look \"thinner\" or \"wider\" depending on resize.\n",
        "            # We resize strict to (256, 256) which is what the UNet expects.\n",
        "            spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear').squeeze(0)\n",
        "            \n",
        "            # Generate\n",
        "            fake_art = generator(spec.unsqueeze(0).to(DEVICE))\n",
        "            \n",
        "            # Convert to Image\n",
        "            img_np = (fake_art.squeeze().permute(1,2,0).cpu().numpy() * 0.5 + 0.5)\n",
        "            img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n",
        "            frames.append(img_np)\n",
        "            \n",
        "    # Write Video Temp\n",
        "    if not frames:\n",
        "        print(\"Audio too short for video generation.\")\n",
        "        return\n",
        "\n",
        "    height, width, _ = frames[0].shape\n",
        "    temp_video = \"temp_video.mp4\" # using .mp4 here directly usually works with 'mp4v' or 'libx264' if avail\n",
        "    \n",
        "    # Option A: Use MoviePy ImageSequenceClip directly (Complete pipeline)\n",
        "    print(\"Compiling video...\")\n",
        "    clip = ImageSequenceClip(frames, fps=fps)\n",
        "    \n",
        "    # Sync Audio\n",
        "    # We used a window of 2s, does the frame correspond to the START of the window or CENTER?\n",
        "    # Usually Center is better for sync. But simpler to just align start.\n",
        "    # The audio clip should match the duration of the processed frames.\n",
        "    video_duration = len(frames) / fps\n",
        "    audio_clip = AudioFileClip(audio_path).subclip(0, video_duration)\n",
        "    \n",
        "    final_clip = clip.set_audio(audio_clip)\n",
        "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "    print(f\"Video saved to {output_path}\")\n",
        "\n",
        "    from IPython.display import Video\n",
        "    display(Video(output_path, embed=True))\n",
        "\n",
        "# Test Video (Create dummy long audio)\n",
        "import scipy.io.wavfile as wav\n",
        "gen = SyntheticAudioGenerator(duration=5.0)\n",
        "audio, _, _ = gen.generate_chirp()\n",
        "wav.write(\"long_test.wav\", SAMPLE_RATE, audio)\n",
        "\n",
        "generate_synesthesia_video(\"long_test.wav\", \"synesthesia_demo.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 3D Visualization (New)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def render_3d_synesthesia(generated_img_tensor):\n",
        "    # generated_img_tensor: Tensor (1, 3, 256, 256) or (3, 256, 256)\n",
        "    if generated_img_tensor.dim() == 4:\n",
        "        generated_img_tensor = generated_img_tensor.squeeze(0)\n",
        "        \n",
        "    # Convert to Numpy [H, W, 3] 0..1\n",
        "    img_np = (generated_img_tensor.permute(1,2,0).cpu().detach().numpy() * 0.5 + 0.5)\n",
        "    \n",
        "    # Define \"Height Map\" as Grayscale Intensity\n",
        "    # R=0.299, G=0.587, B=0.114\n",
        "    z_data = np.dot(img_np[...,:3], [0.299, 0.587, 0.114])\n",
        "    \n",
        "    # For user request \"Map original RGB colors\", we can try to use surfacecolor\n",
        "    # Note: Plotly Surface matches surfacecolor (values) to a colorscale.\n",
        "    # It does NOT easily support direct RGB texture mapping per vertex in Python API without custom work.\n",
        "    # PROXY: We will use the 'Magma' colorscale which closely resembles our synthetic art style.\n",
        "    \n",
        "    fig = go.Figure(data=[go.Surface(z=z_data, colorscale='Magma')])\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title='3D Synesthesia Landscape',\n",
        "        autosize=False,\n",
        "        width=800,\n",
        "        height=800,\n",
        "        margin=dict(l=65, r=50, b=65, t=90),\n",
        "        scene=dict(\n",
        "            xaxis=dict(title='Time'),\n",
        "            yaxis=dict(title='Freq'),\n",
        "            zaxis=dict(title='Intensity'),\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "# Test 3D (using last generated batch item)\n",
        "# gen.eval() ...\n",
        "# render_3d_synesthesia(fake_b[0])\n",
        "print(\"3D Function Loaded. Call render_3d_synesthesia(tensor) to visualize.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}