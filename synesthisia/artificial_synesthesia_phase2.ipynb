{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artificial Synesthesia Phase 2: Professional Generative Art"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\n",
        "import plotly.graph_objects as go\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Drive for Checkpoints\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/Synesthesia_Checkpoints\"\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    print(f\"Checkpoints will be saved to {CHECKPOINT_DIR}\")\n",
        "except:\n",
        "    print(\"Google Drive not mounted. Checkpoints will be saved locally.\")\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "IMG_SIZE = 256\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 1.0 \n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 256 \n",
        "\n",
        "# Hyperparameters Phase 2\n",
        "BATCH_SIZE = 32 if torch.cuda.get_device_properties(0).total_memory > 14e9 else 16\n",
        "LR = 0.0001\n",
        "LAMBDA_L1 = 150\n",
        "EPOCHS = 200\n",
        "START_EPOCH = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Unified Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SpectrogramNormalizer:\n",
        "    @staticmethod\n",
        "    def transform(waveform):\n",
        "        # Waveform -> MelSpectrogram\n",
        "        mel_transform = T.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_fft=N_FFT,\n",
        "            win_length=N_FFT,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_mels=IMG_SIZE,\n",
        "            power=2.0\n",
        "        ).to(waveform.device)\n",
        "        \n",
        "        spec = mel_transform(waveform)\n",
        "        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
        "        \n",
        "        # Strict Normalization: -80dB to 0dB -> [-1, 1]\n",
        "        # (x + 40) / 40 -> range -1 to 1 approximately\n",
        "        spec = (spec + 40) / 40\n",
        "        spec = torch.clamp(spec, -1, 1)\n",
        "        \n",
        "        # Resize to fixed input size (256, 256)\n",
        "        # Note: We interpolate to ensure strict 256x256 input for UNet\n",
        "        if spec.dim() == 2:\n",
        "            spec = spec.unsqueeze(0)\n",
        "        \n",
        "        spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        return spec\n",
        "\n",
        "normalizer = SpectrogramNormalizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Advanced Synthetic Data & Generator (Phase 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AdvancedAudioGenerator(SyntheticAudioGenerator): # Inherits basic methods\n",
        "    def __init__(self, sample_rate=22050, duration=1.0):\n",
        "        self.sr = sample_rate\n",
        "        self.duration = duration\n",
        "        self.n_samples = int(sample_rate * duration)\n",
        "\n",
        "    def generate_sine(self, freq=None): # Re-implementing to be self-contained if class not found\n",
        "        if freq is None:\n",
        "            freq = random.uniform(200, 1000)\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        audio = np.sin(2 * np.pi * freq * t)\n",
        "        return audio.astype(np.float32), \"sine\", freq\n",
        "        \n",
        "    def generate_white_noise(self):\n",
        "        audio = np.random.normal(0, 0.5, self.n_samples)\n",
        "        return audio.astype(np.float32), \"noise\", 0\n",
        "        \n",
        "    def generate_chirp(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        f0 = random.uniform(100, 400)\n",
        "        f1 = random.uniform(800, 1500)\n",
        "        k = (f1 - f0) / self.duration\n",
        "        audio = np.sin(2 * np.pi * (f0 * t + 0.5 * k * t**2))\n",
        "        return audio.astype(np.float32), \"chirp\", (f0, f1)\n",
        "\n",
        "    def generate_fm(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        carrier_freq = random.uniform(200, 800)\n",
        "        mod_freq = random.uniform(10, 100)\n",
        "        mod_index = random.uniform(1, 10)\n",
        "        \n",
        "        audio = np.sin(2 * np.pi * carrier_freq * t + mod_index * np.sin(2 * np.pi * mod_freq * t))\n",
        "        return audio.astype(np.float32), \"fm\", (carrier_freq, mod_freq)\n",
        "\n",
        "    def generate_percussive(self):\n",
        "        # Decay envelope noise\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        noise = np.random.normal(0, 0.8, self.n_samples)\n",
        "        decay = np.exp(-10 * t) # fast decay\n",
        "        audio = noise * decay\n",
        "        return audio.astype(np.float32), \"percussive\", 0\n",
        "\n",
        "class AdvancedVisualGenerator:\n",
        "    def __init__(self, img_size=256):\n",
        "        self.size = img_size\n",
        "\n",
        "    def generate_gradient(self, color_phase=0.0):\n",
        "        x = np.linspace(0, 1, self.size)\n",
        "        y = np.linspace(0, 1, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        R = Y\n",
        "        G = np.sin(Y * np.pi + color_phase) * 0.5 + 0.5\n",
        "        B = 1.0 - Y\n",
        "        return np.stack([R, G, B], axis=-1).astype(np.float32)\n",
        "\n",
        "    def generate_noise_texture(self):\n",
        "        noise = np.random.uniform(0, 1, (self.size, self.size, 3))\n",
        "        noise = np.clip((noise - 0.5) * 2.5 + 0.5, 0, 1) # High contrast\n",
        "        return noise.astype(np.float32)\n",
        "\n",
        "    def generate_structured_pattern(self):\n",
        "        x = np.linspace(0, 10, self.size)\n",
        "        y = np.linspace(0, 10, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = np.sin(X + Y) * 0.5 + 0.5\n",
        "        return np.stack([Z, Z, 1-Z], axis=-1).astype(np.float32)\n",
        "\n",
        "    def generate_voronoi(self):\n",
        "        # Simulate Complex Timbre (FM)\n",
        "        # Create random points\n",
        "        n_points = 20\n",
        "        points = np.random.rand(n_points, 2) * self.size\n",
        "        \n",
        "        # Grid\n",
        "        x = np.arange(self.size)\n",
        "        y = np.arange(self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        \n",
        "        # Calculate distance to nearest point (vectorized is heavy, looping for simplicity/speed trade)\n",
        "        # We'll use a fast approximation: radial blobs\n",
        "        img = np.zeros((self.size, self.size, 3))\n",
        "        for px, py in points:\n",
        "            dist = np.sqrt((X - px)**2 + (Y - py)**2)\n",
        "            # Add blobs\n",
        "            img[:, :, 0] += np.exp(-dist * 0.05)\n",
        "            img[:, :, 1] += np.exp(-dist * 0.03) * np.sin(px)\n",
        "            img[:, :, 2] += np.exp(-dist * 0.04)\n",
        "            \n",
        "        img = np.clip(img, 0, 1)\n",
        "        return img.astype(np.float32)\n",
        "\n",
        "    def generate_fractal_percussive(self):\n",
        "        # Sharp bursts -> Starburst pattern\n",
        "        x = np.linspace(-1, 1, self.size)\n",
        "        y = np.linspace(-1, 1, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        R = np.sqrt(X**2 + Y**2)\n",
        "        A = np.arctan2(Y, X)\n",
        "        \n",
        "        # Starburst\n",
        "        val = np.sin(A * 20) * 0.5 + 0.5\n",
        "        val *= np.exp(-R * 2)\n",
        "        \n",
        "        img = np.stack([val, 1-val, np.random.rand(*val.shape)*val], axis=-1)\n",
        "        return img.astype(np.float32)\n",
        "\n",
        "class SynesthesiaDataset(Dataset):\n",
        "    def __init__(self, size=2000, img_size=256, sample_rate=22050):\n",
        "        self.size = size\n",
        "        self.audio_gen = AdvancedAudioGenerator(sample_rate)\n",
        "        self.vis_gen = AdvancedVisualGenerator(img_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        choice = random.choice([\"sine\", \"noise\", \"chirp\", \"fm\", \"percussive\"])\n",
        "        \n",
        "        if choice == \"sine\":\n",
        "            audio, _, _ = self.audio_gen.generate_sine()\n",
        "            target_img_np = self.vis_gen.generate_gradient(random.random())\n",
        "        elif choice == \"noise\":\n",
        "            audio, _, _ = self.audio_gen.generate_white_noise()\n",
        "            target_img_np = self.vis_gen.generate_noise_texture()\n",
        "        elif choice == \"chirp\":\n",
        "            audio, _, _ = self.audio_gen.generate_chirp()\n",
        "            target_img_np = self.vis_gen.generate_structured_pattern()\n",
        "        elif choice == \"fm\":\n",
        "            audio, _, _ = self.audio_gen.generate_fm()\n",
        "            target_img_np = self.vis_gen.generate_voronoi()\n",
        "        else: # percussive\n",
        "            audio, _, _ = self.audio_gen.generate_percussive()\n",
        "            target_img_np = self.vis_gen.generate_fractal_percussive()\n",
        "\n",
        "        # Audio -> Spectrogram -> Unified Normalize\n",
        "        audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n",
        "        spec = normalizer.transform(audio_tensor)\n",
        "\n",
        "        # Image -> Tensor -> Normalize [-1, 1]\n",
        "        target_img = torch.from_numpy(target_img_np).permute(2, 0, 1)\n",
        "        target_img = (target_img * 2.0) - 1.0\n",
        "\n",
        "        return spec, target_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pix2Pix Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# [Same UNet and PatchGAN code as before to ensure self-contained notebook]\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "        self.final = nn.Sequential(nn.Upsample(scale_factor=2), nn.ZeroPad2d((1, 0, 1, 0)), nn.Conv2d(128, out_channels, 4, padding=1), nn.Tanh())\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x); d2 = self.down2(d1); d3 = self.down3(d2); d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4); d6 = self.down6(d5); d7 = self.down7(d6); d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7); u2 = self.up2(u1, d6); u3 = self.up3(u2, d5); u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3); u6 = self.up6(u5, d2); u7 = self.up7(u6, d1)\n",
        "        return self.final(u7)\n",
        "\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "        def block(in_f, out_f, norm=True):\n",
        "            layers = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1)]\n",
        "            if norm: layers.append(nn.InstanceNorm2d(out_f))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "        self.model = nn.Sequential(\n",
        "            *block(in_channels + 3, 64, norm=False), *block(64, 128),\n",
        "            *block(128, 256), *block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)), nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "    def forward(self, img_A, img_B):\n",
        "        return self.model(torch.cat((img_A, img_B), 1))\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Professional Training Loop (200 Epochs + Checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Helpers\n",
        "def save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D):\n",
        "    path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "    }, path)\n",
        "    print(f\"Checkpoint saved: {path}\")\n",
        "\n",
        "def load_checkpoint(generator, discriminator, optimizer_G, optimizer_D):\n",
        "    # Find latest\n",
        "    files = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"checkpoint\")]\n",
        "    if not files:\n",
        "        return 0\n",
        "    files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest = files[-1]\n",
        "    \n",
        "    checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, latest), map_location=DEVICE)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "    print(f\"Resumed from epoch {checkpoint['epoch']}\")\n",
        "    return checkpoint['epoch'] + 1\n",
        "\n",
        "# Setup\n",
        "generator = UNetGenerator().to(DEVICE)\n",
        "discriminator = PatchGANDiscriminator().to(DEVICE)\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "criterion_GAN = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "criterion_pixelwise = nn.L1Loss().to(DEVICE)\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "\n",
        "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=EPOCHS)\n",
        "scheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=EPOCHS)\n",
        "\n",
        "dataloader = DataLoader(SynesthesiaDataset(size=4000), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "else:\n",
        "    scaler = None\n",
        "\n",
        "# Resume\n",
        "START_EPOCH = load_checkpoint(generator, discriminator, optimizer_G, optimizer_D)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "    for i, (spec, target_img) in enumerate(dataloader):\n",
        "        real_a = spec.to(DEVICE)\n",
        "        real_b = target_img.to(DEVICE)\n",
        "\n",
        "        # Train G\n",
        "        optimizer_G.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                fake_b = generator(real_a)\n",
        "                pred_fake = discriminator(fake_b, real_a)\n",
        "                valid = torch.ones_like(pred_fake)\n",
        "                loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "                loss_pixel = criterion_pixelwise(fake_b, real_b)\n",
        "                loss_G = loss_GAN + LAMBDA_L1 * loss_pixel\n",
        "            scaler.scale(loss_G).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            fake_b = generator(real_a)\n",
        "            pred_fake = discriminator(fake_b, real_a)\n",
        "            valid = torch.ones_like(pred_fake)\n",
        "            loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "            loss_pixel = criterion_pixelwise(fake_b, real_b)\n",
        "            loss_G = loss_GAN + LAMBDA_L1 * loss_pixel\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # Train D\n",
        "        optimizer_D.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                pred_real = discriminator(real_b, real_a)\n",
        "                valid = torch.ones_like(pred_real)\n",
        "                loss_real = criterion_GAN(pred_real, valid)\n",
        "                pred_fake = discriminator(fake_b.detach(), real_a)\n",
        "                fake = torch.zeros_like(pred_fake)\n",
        "                loss_fake = criterion_GAN(pred_fake, fake)\n",
        "                loss_D = 0.5 * (loss_real + loss_fake)\n",
        "            scaler.scale(loss_D).backward()\n",
        "            scaler.step(optimizer_D)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            pred_real = discriminator(real_b, real_a)\n",
        "            loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
        "            pred_fake = discriminator(fake_b.detach(), real_a)\n",
        "            loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
        "            loss_D = 0.5 * (loss_real + loss_fake)\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[Epoch {epoch}] [Batch {i}] [Loss D: {loss_D.item():.4f}] [Loss G: {loss_G.item():.4f}]\")\n",
        "\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    # Save and Show\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D)\n",
        "        \n",
        "    # Visualize 3x3 Grid\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            fakes = generator(real_a[:3])\n",
        "            # Show simplified\n",
        "            plt.figure(figsize=(9, 9))\n",
        "            for k in range(3):\n",
        "                plt.subplot(3, 3, k*3+1)\n",
        "                plt.imshow(real_a[k].cpu().squeeze(), cmap='magma', origin='lower'); plt.axis('off')\n",
        "                plt.subplot(3, 3, k*3+2)\n",
        "                plt.imshow(real_b[k].cpu().permute(1,2,0) * 0.5 + 0.5); plt.axis('off')\n",
        "                plt.subplot(3, 3, k*3+3)\n",
        "                plt.imshow(fakes[k].cpu().permute(1,2,0) * 0.5 + 0.5); plt.axis('off')\n",
        "            plt.show()\n",
        "        generator.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Kinetic 3D Experience (Final Demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_kinetic_video(audio_path, output_path=\"kinetic_synesthesia.mp4\", fps=30):\n",
        "    print(f\"Rendering Kinetic 3D Experience for {audio_path}...\")\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if sr != SAMPLE_RATE:\n",
        "        waveform = T.Resample(sr, SAMPLE_RATE)(waveform)\n",
        "    \n",
        "    window_duration = 2.0\n",
        "    window_samples = int(window_duration * SAMPLE_RATE)\n",
        "    step_samples = int(SAMPLE_RATE / fps)\n",
        "    \n",
        "    generator.eval()\n",
        "    \n",
        "    # Pre-setup Matplotlib logic\n",
        "    # We want a FIG with 2 subplots: Left (Image), Right (3D Wireframe)\n",
        "    \n",
        "    temp_frames = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for start in range(0, waveform.shape[1] - window_samples, step_samples):\n",
        "            chunk = waveform[:, start:start+window_samples]\n",
        "            \n",
        "            # 1. Spec & Gen\n",
        "            spec = normalizer.transform(chunk)\n",
        "            fake_tensor = generator(spec.unsqueeze(0).to(DEVICE))\n",
        "            \n",
        "            # 2. Get Data for Plotting\n",
        "            img_np = (fake_tensor.squeeze().permute(1,2,0).cpu().numpy() * 0.5 + 0.5)\n",
        "            img_np = np.clip(img_np, 0, 1) # Keep float for plot\n",
        "            \n",
        "            # Grayscale Height Map (Low Res for 3D speed)\n",
        "            # Resize img_np to 64x64 for wireframe speed\n",
        "            h_map_small = cv2.resize(img_np, (64, 64))\n",
        "            Z = np.dot(h_map_small[..., :3], [0.299, 0.587, 0.114])\n",
        "            X, Y = np.meshgrid(np.linspace(0, 1, 64), np.linspace(0, 1, 64))\n",
        "            \n",
        "            # 3. Render Frame with Matplotlib\n",
        "            fig = plt.figure(figsize=(10, 5), dpi=100)\n",
        "            \n",
        "            # Left: Art\n",
        "            ax2d = fig.add_subplot(1, 2, 1)\n",
        "            ax2d.imshow(img_np)\n",
        "            ax2d.set_title(\"Synesthesia Art\")\n",
        "            ax2d.axis('off')\n",
        "            \n",
        "            # Right: 3D Pulse\n",
        "            ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "            # Wireframe or Surface\n",
        "            surf = ax3d.plot_surface(X, Y, Z, cmap='magma', linewidth=0, antialiased=False)\n",
        "            ax3d.set_zlim(0, 1)\n",
        "            ax3d.view_init(elev=45, azim=start/1000) # Simple rotation effect\n",
        "            ax3d.set_title(\"Kinetic Topography\")\n",
        "            ax3d.axis('off')\n",
        "            \n",
        "            # Save to buffer\n",
        "            fig.canvas.draw()\n",
        "            frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "            frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            temp_frames.append(frame)\n",
        "            \n",
        "            plt.close(fig)\n",
        "            if len(temp_frames) % 50 == 0:\n",
        "                print(f\"Rendered {len(temp_frames)} frames...\")\n",
        "    \n",
        "    if not temp_frames:\n",
        "        return\n",
        "        \n",
        "    print(\"Encoding video...\")\n",
        "    clip = ImageSequenceClip(temp_frames, fps=fps)\n",
        "    audio_clip = AudioFileClip(audio_path).subclip(0, len(temp_frames)/fps)\n",
        "    final = clip.set_audio(audio_clip)\n",
        "    final.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "    \n",
        "    from IPython.display import Video\n",
        "    display(Video(output_path, embed=True))\n",
        "\n",
        "# Create a test file and run\n",
        "gen = AdvancedAudioGenerator(duration=5.0)\n",
        "audio, _, _ = gen.generate_fm()\n",
        "import scipy.io.wavfile as wav\n",
        "wav.write(\"kinetics_test.wav\", SAMPLE_RATE, audio)\n",
        "\n",
        "generate_kinetic_video(\"kinetics_test.wav\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}