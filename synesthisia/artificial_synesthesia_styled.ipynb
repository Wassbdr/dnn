{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artificial Synesthesia Phase 2 (Styled): Professional Generative Art"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Optional**: Upload a file named `style.jpg` (texture/canvas/paper) to the root directory or Google Drive for custom domain injection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\n",
        "import plotly.graph_objects as go\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Drive for Checkpoints\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/Synesthesia_Checkpoints\"\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "except:\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "IMG_SIZE = 256\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 1.0 \n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 256\n",
        "BATCH_SIZE = 32 if torch.cuda.get_device_properties(0).total_memory > 14e9 else 16\n",
        "LR = 0.0001\n",
        "LAMBDA_L1 = 150\n",
        "EPOCHS = 200\n",
        "START_EPOCH = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Unified Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SpectrogramNormalizer:\n",
        "    @staticmethod\n",
        "    def transform(waveform):\n",
        "        mel_transform = T.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_fft=N_FFT,\n",
        "            win_length=N_FFT,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_mels=IMG_SIZE,\n",
        "            power=2.0\n",
        "        ).to(waveform.device)\n",
        "        spec = mel_transform(waveform)\n",
        "        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
        "        spec = (spec + 40) / 40\n",
        "        spec = torch.clamp(spec, -1, 1)\n",
        "        if spec.dim() == 2:\n",
        "            spec = spec.unsqueeze(0)\n",
        "        spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        return spec\n",
        "normalizer = SpectrogramNormalizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Style-Injected Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- STYLE INJECTOR ---\n",
        "class StyleInjector:\n",
        "    def __init__(self, size=256, texture_path=\"texture.jpg\"):\n",
        "        self.size = size\n",
        "        self.texture = self.load_or_generate_texture(texture_path)\n",
        "    \n",
        "    def load_or_generate_texture(self, path):\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Loading custom style texture from {path}\")\n",
        "            try:\n",
        "                img = Image.open(path).convert('RGB').resize((self.size, self.size))\n",
        "                # Normalize 0-1\n",
        "                return np.array(img).astype(np.float32) / 255.0\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading texture: {e}\")\n",
        "        \n",
        "        print(\"Using Procedural Paper Texture fallback.\")\n",
        "        return self.generate_paper_texture()\n",
        "\n",
        "    def generate_paper_texture(self):\n",
        "        # Generate grainy paper noise\n",
        "        noise = np.random.normal(0.95, 0.05, (self.size, self.size, 3))\n",
        "        # Add some 'fiber' lines\n",
        "        for _ in range(20):\n",
        "            x1, y1 = np.random.randint(0, self.size, 2)\n",
        "            length = np.random.randint(5, 20)\n",
        "            angle = np.random.uniform(0, 360)\n",
        "            x2 = int(x1 + length * np.cos(np.radians(angle)))\n",
        "            y2 = int(y1 + length * np.sin(np.radians(angle)))\n",
        "            cv2.line(noise, (x1, y1), (x2, y2), (0.8, 0.8, 0.7), 1)\n",
        "        return np.clip(noise, 0, 1).astype(np.float32)\n",
        "\n",
        "    def apply_style(self, generated_img):\n",
        "        # Blend Mode: Multiply for watercolor effect\n",
        "        # generated_img is 0-1 RGB\n",
        "        return np.clip(generated_img * self.texture, 0, 1).astype(np.float32)\n",
        "\n",
        "# --- AUDIO & VISUAL GENS ---\n",
        "class AdvancedAudioGenerator:\n",
        "    def __init__(self, sample_rate=22050, duration=1.0):\n",
        "        self.sr = sample_rate\n",
        "        self.duration = duration\n",
        "        self.n_samples = int(sample_rate * duration)\n",
        "\n",
        "    def generate_sine(self, freq=None): \n",
        "        if freq is None: freq = random.uniform(200, 1000)\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        audio = np.sin(2 * np.pi * freq * t)\n",
        "        return audio.astype(np.float32), \"sine\", freq\n",
        "        \n",
        "    def generate_white_noise(self):\n",
        "        audio = np.random.normal(0, 0.5, self.n_samples)\n",
        "        return audio.astype(np.float32), \"noise\", 0\n",
        "        \n",
        "    def generate_chirp(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        f0 = random.uniform(100, 400); f1 = random.uniform(800, 1500)\n",
        "        k = (f1 - f0) / self.duration\n",
        "        audio = np.sin(2 * np.pi * (f0 * t + 0.5 * k * t**2))\n",
        "        return audio.astype(np.float32), \"chirp\", (f0, f1)\n",
        "\n",
        "    def generate_fm(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        cf = random.uniform(200, 800); mf = random.uniform(10, 100); mi = random.uniform(1, 10)\n",
        "        audio = np.sin(2 * np.pi * cf * t + mi * np.sin(2 * np.pi * mf * t))\n",
        "        return audio.astype(np.float32), \"fm\", (cf, mf)\n",
        "\n",
        "    def generate_percussive(self):\n",
        "        t = np.linspace(0, self.duration, self.n_samples)\n",
        "        noise = np.random.normal(0, 0.8, self.n_samples)\n",
        "        decay = np.exp(-10 * t) \n",
        "        audio = noise * decay\n",
        "        return audio.astype(np.float32), \"percussive\", 0\n",
        "\n",
        "class AdvancedVisualGenerator:\n",
        "    def __init__(self, img_size=256):\n",
        "        self.size = img_size\n",
        "\n",
        "    def generate_gradient(self, color_phase=0.0):\n",
        "        x = np.linspace(0, 1, self.size); y = np.linspace(0, 1, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        R = Y; G = np.sin(Y * np.pi + color_phase) * 0.5 + 0.5; B = 1.0 - Y\n",
        "        return np.stack([R, G, B], axis=-1).astype(np.float32)\n",
        "\n",
        "    def generate_noise_texture(self):\n",
        "        noise = np.random.uniform(0, 1, (self.size, self.size, 3))\n",
        "        noise = np.clip((noise - 0.5) * 2.5 + 0.5, 0, 1) \n",
        "        return noise.astype(np.float32)\n",
        "\n",
        "    def generate_structured_pattern(self):\n",
        "        x = np.linspace(0, 10, self.size); y = np.linspace(0, 10, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = np.sin(X + Y) * 0.5 + 0.5\n",
        "        return np.stack([Z, Z, 1-Z], axis=-1).astype(np.float32)\n",
        "\n",
        "    def generate_voronoi(self):\n",
        "        n_points = 20\n",
        "        points = np.random.rand(n_points, 2) * self.size\n",
        "        x = np.arange(self.size); y = np.arange(self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        img = np.zeros((self.size, self.size, 3))\n",
        "        for px, py in points:\n",
        "            dist = np.sqrt((X - px)**2 + (Y - py)**2)\n",
        "            img[:, :, 0] += np.exp(-dist * 0.05)\n",
        "            img[:, :, 1] += np.exp(-dist * 0.03) * np.sin(px)\n",
        "            img[:, :, 2] += np.exp(-dist * 0.04)\n",
        "        return np.clip(img, 0, 1).astype(np.float32)\n",
        "\n",
        "    def generate_fractal_percussive(self):\n",
        "        x = np.linspace(-1, 1, self.size); y = np.linspace(-1, 1, self.size)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        R = np.sqrt(X**2 + Y**2); A = np.arctan2(Y, X)\n",
        "        val = np.sin(A * 20) * 0.5 + 0.5\n",
        "        val *= np.exp(-R * 2)\n",
        "        img = np.stack([val, 1-val, np.random.rand(*val.shape)*val], axis=-1)\n",
        "        return img.astype(np.float32)\n",
        "\n",
        "class SynesthesiaDataset(Dataset):\n",
        "    def __init__(self, size=2000, img_size=256, sample_rate=22050, style_file=\"style.jpg\"):\n",
        "        self.size = size\n",
        "        self.audio_gen = AdvancedAudioGenerator(sample_rate)\n",
        "        self.vis_gen = AdvancedVisualGenerator(img_size)\n",
        "        self.style_injector = StyleInjector(img_size, style_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        choice = random.choice([\"sine\", \"noise\", \"chirp\", \"fm\", \"percussive\"])\n",
        "        \n",
        "        if choice == \"sine\":\n",
        "            audio, _, _ = self.audio_gen.generate_sine()\n",
        "            raw_img = self.vis_gen.generate_gradient(random.random())\n",
        "        elif choice == \"noise\":\n",
        "            audio, _, _ = self.audio_gen.generate_white_noise()\n",
        "            raw_img = self.vis_gen.generate_noise_texture()\n",
        "        elif choice == \"chirp\":\n",
        "            audio, _, _ = self.audio_gen.generate_chirp()\n",
        "            raw_img = self.vis_gen.generate_structured_pattern()\n",
        "        elif choice == \"fm\":\n",
        "            audio, _, _ = self.audio_gen.generate_fm()\n",
        "            raw_img = self.vis_gen.generate_voronoi()\n",
        "        else: \n",
        "            audio, _, _ = self.audio_gen.generate_percussive()\n",
        "            raw_img = self.vis_gen.generate_fractal_percussive()\n",
        "\n",
        "        # APPLY STYLE\n",
        "        styled_img_np = self.style_injector.apply_style(raw_img)\n",
        "\n",
        "        # Process\n",
        "        audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n",
        "        spec = normalizer.transform(audio_tensor)\n",
        "\n",
        "        target_img = torch.from_numpy(styled_img_np).permute(2, 0, 1)\n",
        "        target_img = (target_img * 2.0) - 1.0 # 0..1 to -1..1\n",
        "\n",
        "        return spec, target_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pix2Pix Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_c, out_c, norm=True, drop=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n",
        "        if norm: layers.append(nn.InstanceNorm2d(out_c))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if drop: layers.append(nn.Dropout(drop))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_c, out_c, drop=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False), nn.InstanceNorm2d(out_c), nn.ReLU(True)]\n",
        "        if drop: layers.append(nn.Dropout(drop))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x, skip):\n",
        "        x = self.model(x)\n",
        "        # Check shapes just in case (debug safety)\n",
        "        if x.shape != skip.shape:\n",
        "             # This can happen if padding was needed in Down path. \n",
        "             # Resize x to match skip\n",
        "             x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear')\n",
        "        return torch.cat((x, skip), 1)\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_c=1, out_c=3):\n",
        "        super().__init__()\n",
        "        self.d1=UNetDown(in_c,64,norm=False); self.d2=UNetDown(64,128); self.d3=UNetDown(128,256)\n",
        "        self.d4=UNetDown(256,512,drop=0.5); self.d5=UNetDown(512,512,drop=0.5); self.d6=UNetDown(512,512,drop=0.5)\n",
        "        self.d7=UNetDown(512,512,drop=0.5); self.d8=UNetDown(512,512,norm=False,drop=0.5)\n",
        "        self.u1=UNetUp(512,512,drop=0.5); self.u2=UNetUp(1024,512,drop=0.5); self.u3=UNetUp(1024,512,drop=0.5)\n",
        "        self.u4=UNetUp(1024,512,drop=0.5); self.u5=UNetUp(1024,256); self.u6=UNetUp(512,128); self.u7=UNetUp(256,64)\n",
        "        self.final=nn.Sequential(nn.Upsample(scale_factor=2), nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(128,out_c,4,padding=1), nn.Tanh())\n",
        "    def forward(self, x):\n",
        "        d1=self.d1(x); d2=self.d2(d1); d3=self.d3(d2); d4=self.d4(d3); d5=self.d5(d4); d6=self.d6(d5); d7=self.d7(d6); d8=self.d8(d7)\n",
        "        u1=self.u1(d8,d7); u2=self.u2(u1,d6); u3=self.u3(u2,d5); u4=self.u4(u3,d4); u5=self.u5(u4,d3); u6=self.u6(u5,d2); u7=self.u7(u6,d1)\n",
        "        return self.final(u7)\n",
        "\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_c=1):\n",
        "        super().__init__()\n",
        "        def block(i, o, n=True):\n",
        "            l = [nn.Conv2d(i, o, 4, 2, 1)]; \n",
        "            if n: l.append(nn.InstanceNorm2d(o)); \n",
        "            l.append(nn.LeakyReLU(0.2, True)); return l\n",
        "        self.model = nn.Sequential(*block(in_c+3,64,False), *block(64,128), *block(128,256), *block(256,512), \n",
        "                                   nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(512,1,4,padding=1,bias=False))\n",
        "    def forward(self, a, b): return self.model(torch.cat((a, b), 1))\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight.data, 1.0, 0.02); torch.nn.init.constant_(m.bias.data, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Professional Training (Styled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D):\n",
        "    path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "    torch.save({'epoch': epoch, 'G': generator.state_dict(), 'D': discriminator.state_dict(), 'optG': optimizer_G.state_dict(), 'optD': optimizer_D.state_dict()}, path)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_checkpoint(generator, discriminator, optimizer_G, optimizer_D):\n",
        "    files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"checkpoint\")], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    if not files: return 0\n",
        "    cp = torch.load(os.path.join(CHECKPOINT_DIR, files[-1]), map_location=DEVICE)\n",
        "    generator.load_state_dict(cp['G']); discriminator.load_state_dict(cp['D']); optimizer_G.load_state_dict(cp['optG']); optimizer_D.load_state_dict(cp['optD'])\n",
        "    print(f\"Resumed epoch {cp['epoch']}\"); return cp['epoch'] + 1\n",
        "\n",
        "generator = UNetGenerator().to(DEVICE); discriminator = PatchGANDiscriminator().to(DEVICE)\n",
        "generator.apply(weights_init_normal); discriminator.apply(weights_init_normal)\n",
        "criterion_GAN = nn.BCEWithLogitsLoss().to(DEVICE); criterion_L1 = nn.L1Loss().to(DEVICE)\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=EPOCHS)\n",
        "scheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=EPOCHS)\n",
        "\n",
        "# Attempt to look for 'style.jpg' in the current directory or Drive\n",
        "style_path = \"style.jpg\"\n",
        "if not os.path.exists(style_path) and os.path.exists(\"/content/drive/MyDrive/style.jpg\"):\n",
        "    style_path = \"/content/drive/MyDrive/style.jpg\"\n",
        "\n",
        "print(f\"Initializing Dataset with style check on: {style_path}\")\n",
        "dataloader = DataLoader(SynesthesiaDataset(size=4000, style_file=style_path), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None\n",
        "\n",
        "START_EPOCH = load_checkpoint(generator, discriminator, optimizer_G, optimizer_D)\n",
        "\n",
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "    for i, (spec, real_b) in enumerate(dataloader):\n",
        "        real_a, real_b = spec.to(DEVICE), real_b.to(DEVICE)\n",
        "        \n",
        "        # Train G\n",
        "        optimizer_G.zero_grad()\n",
        "        with torch.amp.autocast('cuda') if scaler else torch.no_grad(): # no_grad is dummy for CPU fallthrough in this ternary context, manually handled below\n",
        "             if scaler:\n",
        "                fake_b = generator(real_a)\n",
        "                pred_fake = discriminator(fake_b, real_a)\n",
        "                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n",
        "             else:\n",
        "                fake_b = generator(real_a)\n",
        "                pred_fake = discriminator(fake_b, real_a)\n",
        "                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n",
        "        \n",
        "        if scaler: scaler.scale(loss_G).backward(); scaler.step(optimizer_G); scaler.update()\n",
        "        else: loss_G.backward(); optimizer_G.step()\n",
        "\n",
        "        # Train D\n",
        "        optimizer_D.zero_grad()\n",
        "        with torch.amp.autocast('cuda') if scaler else torch.no_grad():\n",
        "             if scaler:\n",
        "                pred_real = discriminator(real_b, real_a)\n",
        "                pred_fake = discriminator(fake_b.detach(), real_a)\n",
        "                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n",
        "             else:\n",
        "                pred_real = discriminator(real_b, real_a)\n",
        "                pred_fake = discriminator(fake_b.detach(), real_a)\n",
        "                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n",
        "\n",
        "        if scaler: scaler.scale(loss_D).backward(); scaler.step(optimizer_D); scaler.update()\n",
        "        else: loss_D.backward(); optimizer_D.step()\n",
        "\n",
        "        if i % 100 == 0: print(f\"E{epoch} B{i} L_D:{loss_D.item():.3f} L_G:{loss_G.item():.3f}\")\n",
        "\n",
        "    scheduler_G.step(); scheduler_D.step()\n",
        "    if (epoch+1) % 10 == 0: save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D)\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            f = generator(real_a[:3])\n",
        "            plt.figure(figsize=(9,9))\n",
        "            for k in range(3):\n",
        "                plt.subplot(3,3,k*3+1); plt.imshow(real_a[k].cpu().squeeze(),cmap='magma',origin='lower'); plt.axis('off')\n",
        "                plt.subplot(3,3,k*3+2); plt.imshow(real_b[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n",
        "                plt.subplot(3,3,k*3+3); plt.imshow(f[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n",
        "            plt.show()\n",
        "        generator.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Kinetic 3D Experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_kinetic_video(audio_path, output_path=\"kinetic_synesthesia.mp4\", fps=30):\n",
        "    print(f\"Rendering Kinetic 3D Experience for {audio_path}...\")\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if sr != SAMPLE_RATE: waveform = T.Resample(sr, SAMPLE_RATE)(waveform)\n",
        "    \n",
        "    window_samples = int(2.0 * SAMPLE_RATE); step_samples = int(SAMPLE_RATE / fps)\n",
        "    generator.eval(); temp_frames = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for start in range(0, waveform.shape[1] - window_samples, step_samples):\n",
        "            chunk = waveform[:, start:start+window_samples]\n",
        "            spec = normalizer.transform(chunk)\n",
        "            fake_tensor = generator(spec.unsqueeze(0).to(DEVICE))\n",
        "            img_np = np.clip((fake_tensor.squeeze().permute(1,2,0).cpu().numpy()*0.5+0.5), 0, 1)\n",
        "            \n",
        "            # Simple Matplotlib Render\n",
        "            h_map = cv2.resize(img_np, (64, 64))\n",
        "            Z = np.dot(h_map[..., :3], [0.299, 0.587, 0.114])\n",
        "            X, Y = np.meshgrid(np.linspace(0,1,64), np.linspace(0,1,64))\n",
        "            \n",
        "            fig = plt.figure(figsize=(10, 5), dpi=80)\n",
        "            ax2d = fig.add_subplot(1, 2, 1); ax2d.imshow(img_np); ax2d.axis('off'); ax2d.set_title(\"Styled Art\")\n",
        "            ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "            ax3d.plot_surface(X, Y, Z, cmap='magma', linewidth=0, antialiased=False)\n",
        "            ax3d.set_zlim(0, 1); ax3d.view_init(elev=45, azim=start/1000); ax3d.axis('off'); ax3d.set_title(\"Topography\")\n",
        "            \n",
        "            fig.canvas.draw()\n",
        "            frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "            frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            temp_frames.append(frame); plt.close(fig)\n",
        "            if len(temp_frames) % 50 == 0: print(f\"Rendered {len(temp_frames)} frames\")\n",
        "    \n",
        "    if temp_frames:\n",
        "        clip = ImageSequenceClip(temp_frames, fps=fps)\n",
        "        audio = AudioFileClip(audio_path).subclip(0, len(temp_frames)/fps)\n",
        "        clip.set_audio(audio).write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "        from IPython.display import Video\n",
        "        display(Video(output_path, embed=True))\n",
        "\n",
        "# Test\n",
        "gen = AdvancedAudioGenerator(duration=5.0)\n",
        "audio, _, _ = gen.generate_fm()\n",
        "import scipy.io.wavfile as wav\n",
        "wav.write(\"kinetics_test.wav\", SAMPLE_RATE, audio)\n",
        "generate_kinetic_video(\"kinetics_test.wav\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}