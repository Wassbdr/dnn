{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14542704,"sourceType":"datasetVersion","datasetId":9288379},{"sourceId":14542740,"sourceType":"datasetVersion","datasetId":9288403},{"sourceId":14546904,"sourceType":"datasetVersion","datasetId":9291107}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Synesthesia (Kaggle Edition)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup & Config","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport torchaudio.transforms as T\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport random\nimport os\nimport cv2\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\nimport plotly.graph_objects as go\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport shutil\n\n# Environment Detection\nif os.path.exists('/kaggle/input'):\n    print(\"Kaggle Environment Detected\")\n    CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\nelse:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        CHECKPOINT_DIR = \"/content/drive/MyDrive/Synesthesia_Checkpoints\"\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    except:\n        CHECKPOINT_DIR = \"./checkpoints\"\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nprint(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nIMG_SIZE = 256\nSAMPLE_RATE = 22050\nDURATION = 1.0 \nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 256\nBATCH_SIZE = 32 if torch.cuda.get_device_properties(0).total_memory > 14e9 else 16\nLR = 0.0001\nLAMBDA_L1 = 150\nEPOCHS = 200\nSTART_EPOCH = 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:34:51.626784Z","iopub.execute_input":"2026-01-19T10:34:51.627143Z","iopub.status.idle":"2026-01-19T10:34:51.637311Z","shell.execute_reply.started":"2026-01-19T10:34:51.627113Z","shell.execute_reply":"2026-01-19T10:34:51.636518Z"}},"outputs":[{"name":"stdout","text":"Kaggle Environment Detected\nCheckpoints will be saved to: /kaggle/working/checkpoints\nUsing device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 2. Unified Normalization","metadata":{}},{"cell_type":"code","source":"\nclass SpectrogramNormalizer:\n    @staticmethod\n    def transform(waveform):\n        mel_transform = T.MelSpectrogram(\n            sample_rate=SAMPLE_RATE,\n            n_fft=N_FFT,\n            win_length=N_FFT,\n            hop_length=HOP_LENGTH,\n            n_mels=IMG_SIZE,\n            power=2.0\n        ).to(waveform.device)\n        spec = mel_transform(waveform)\n        spec = torchaudio.transforms.AmplitudeToDB()(spec)\n        spec = (spec + 40) / 40\n        spec = torch.clamp(spec, -1, 1)\n        if spec.dim() == 2:\n            spec = spec.unsqueeze(0)\n        spec = torch.nn.functional.interpolate(spec.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n        return spec\nnormalizer = SpectrogramNormalizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:34:51.638631Z","iopub.execute_input":"2026-01-19T10:34:51.639435Z","iopub.status.idle":"2026-01-19T10:34:51.653433Z","shell.execute_reply.started":"2026-01-19T10:34:51.639406Z","shell.execute_reply":"2026-01-19T10:34:51.652506Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3. Style-Injected Dataset","metadata":{}},{"cell_type":"code","source":"\n# --- STYLE INJECTOR ---\nclass StyleInjector:\n    def __init__(self, size=256, texture_path=\"style.jpg\"):\n        self.size = size\n        self.texture = self.load_or_generate_texture(texture_path)\n    \n    def load_or_generate_texture(self, path):\n        # Check current dir, then Kaggle Input, then Drive\n        possible_paths = [\n            path,\n            f\"/kaggle/input/{path}\",\n            f\"/kaggle/working/{path}\",\n            f\"/content/drive/MyDrive/{path}\"\n        ]\n        \n        found_path = None\n        for p in possible_paths:\n            if os.path.exists(p):\n                found_path = p\n                break\n        \n        if found_path:\n            print(f\"Loading custom style texture from {found_path}\")\n            try:\n                img = Image.open(found_path).convert('RGB').resize((self.size, self.size))\n                return np.array(img).astype(np.float32) / 255.0\n            except Exception as e:\n                print(f\"Error loading texture: {e}\")\n        \n        print(\"Using Procedural Paper Texture fallback.\")\n        return self.generate_paper_texture()\n\n    def generate_paper_texture(self):\n        noise = np.random.normal(0.95, 0.05, (self.size, self.size, 3))\n        for _ in range(20):\n            x1, y1 = np.random.randint(0, self.size, 2)\n            length = np.random.randint(5, 20)\n            angle = np.random.uniform(0, 360)\n            x2 = int(x1 + length * np.cos(np.radians(angle)))\n            y2 = int(y1 + length * np.sin(np.radians(angle)))\n            cv2.line(noise, (x1, y1), (x2, y2), (0.8, 0.8, 0.7), 1)\n        return np.clip(noise, 0, 1).astype(np.float32)\n\n    def apply_style(self, generated_img):\n        return np.clip(generated_img * self.texture, 0, 1).astype(np.float32)\n\n# --- AUDIO & VISUAL GENS ---\nclass AdvancedAudioGenerator:\n    def __init__(self, sample_rate=22050, duration=1.0):\n        self.sr = sample_rate\n        self.duration = duration\n        self.n_samples = int(sample_rate * duration)\n\n    def generate_sine(self, freq=None): \n        if freq is None: freq = random.uniform(200, 1000)\n        t = np.linspace(0, self.duration, self.n_samples)\n        audio = np.sin(2 * np.pi * freq * t)\n        return audio.astype(np.float32), \"sine\", freq\n        \n    def generate_white_noise(self):\n        audio = np.random.normal(0, 0.5, self.n_samples)\n        return audio.astype(np.float32), \"noise\", 0\n        \n    def generate_chirp(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        f0 = random.uniform(100, 400); f1 = random.uniform(800, 1500)\n        k = (f1 - f0) / self.duration\n        audio = np.sin(2 * np.pi * (f0 * t + 0.5 * k * t**2))\n        return audio.astype(np.float32), \"chirp\", (f0, f1)\n\n    def generate_fm(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        cf = random.uniform(200, 800); mf = random.uniform(10, 100); mi = random.uniform(1, 10)\n        audio = np.sin(2 * np.pi * cf * t + mi * np.sin(2 * np.pi * mf * t))\n        return audio.astype(np.float32), \"fm\", (cf, mf)\n\n    def generate_percussive(self):\n        t = np.linspace(0, self.duration, self.n_samples)\n        noise = np.random.normal(0, 0.8, self.n_samples)\n        decay = np.exp(-10 * t) \n        audio = noise * decay\n        return audio.astype(np.float32), \"percussive\", 0\n\nclass AdvancedVisualGenerator:\n    def __init__(self, img_size=256):\n        self.size = img_size\n\n    def generate_gradient(self, color_phase=0.0):\n        x = np.linspace(0, 1, self.size); y = np.linspace(0, 1, self.size)\n        X, Y = np.meshgrid(x, y)\n        R = Y; G = np.sin(Y * np.pi + color_phase) * 0.5 + 0.5; B = 1.0 - Y\n        return np.stack([R, G, B], axis=-1).astype(np.float32)\n\n    def generate_noise_texture(self):\n        noise = np.random.uniform(0, 1, (self.size, self.size, 3))\n        noise = np.clip((noise - 0.5) * 2.5 + 0.5, 0, 1) \n        return noise.astype(np.float32)\n\n    def generate_structured_pattern(self):\n        x = np.linspace(0, 10, self.size); y = np.linspace(0, 10, self.size)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(X + Y) * 0.5 + 0.5\n        return np.stack([Z, Z, 1-Z], axis=-1).astype(np.float32)\n\n    def generate_voronoi(self):\n        n_points = 20\n        points = np.random.rand(n_points, 2) * self.size\n        x = np.arange(self.size); y = np.arange(self.size)\n        X, Y = np.meshgrid(x, y)\n        img = np.zeros((self.size, self.size, 3))\n        for px, py in points:\n            dist = np.sqrt((X - px)**2 + (Y - py)**2)\n            img[:, :, 0] += np.exp(-dist * 0.05)\n            img[:, :, 1] += np.exp(-dist * 0.03) * np.sin(px)\n            img[:, :, 2] += np.exp(-dist * 0.04)\n        return np.clip(img, 0, 1).astype(np.float32)\n\n    def generate_fractal_percussive(self):\n        x = np.linspace(-1, 1, self.size); y = np.linspace(-1, 1, self.size)\n        X, Y = np.meshgrid(x, y)\n        R = np.sqrt(X**2 + Y**2); A = np.arctan2(Y, X)\n        val = np.sin(A * 20) * 0.5 + 0.5\n        val *= np.exp(-R * 2)\n        img = np.stack([val, 1-val, np.random.rand(*val.shape)*val], axis=-1)\n        return img.astype(np.float32)\n\nclass SynesthesiaDataset(Dataset):\n    def __init__(self, size=2000, img_size=256, sample_rate=22050, style_file=\"style.jpg\"):\n        self.size = size\n        self.audio_gen = AdvancedAudioGenerator(sample_rate)\n        self.vis_gen = AdvancedVisualGenerator(img_size)\n        self.style_injector = StyleInjector(img_size, style_file)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        choice = random.choice([\"sine\", \"noise\", \"chirp\", \"fm\", \"percussive\"])\n        \n        if choice == \"sine\":\n            audio, _, _ = self.audio_gen.generate_sine()\n            raw_img = self.vis_gen.generate_gradient(random.random())\n        elif choice == \"noise\":\n            audio, _, _ = self.audio_gen.generate_white_noise()\n            raw_img = self.vis_gen.generate_noise_texture()\n        elif choice == \"chirp\":\n            audio, _, _ = self.audio_gen.generate_chirp()\n            raw_img = self.vis_gen.generate_structured_pattern()\n        elif choice == \"fm\":\n            audio, _, _ = self.audio_gen.generate_fm()\n            raw_img = self.vis_gen.generate_voronoi()\n        else: \n            audio, _, _ = self.audio_gen.generate_percussive()\n            raw_img = self.vis_gen.generate_fractal_percussive()\n\n        # APPLY STYLE\n        styled_img_np = self.style_injector.apply_style(raw_img)\n\n        # Process\n        audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n        spec = normalizer.transform(audio_tensor)\n\n        target_img = torch.from_numpy(styled_img_np).permute(2, 0, 1)\n        target_img = (target_img * 2.0) - 1.0 # 0..1 to -1..1\n\n        return spec, target_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:34:51.729952Z","iopub.execute_input":"2026-01-19T10:34:51.730406Z","iopub.status.idle":"2026-01-19T10:34:51.758574Z","shell.execute_reply.started":"2026-01-19T10:34:51.730372Z","shell.execute_reply":"2026-01-19T10:34:51.757697Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Pix2Pix Model","metadata":{}},{"cell_type":"code","source":"\nclass UNetDown(nn.Module):\n    def __init__(self, in_c, out_c, norm=True, drop=0.0):\n        super().__init__()\n        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n        if norm: layers.append(nn.InstanceNorm2d(out_c))\n        layers.append(nn.LeakyReLU(0.2))\n        if drop: layers.append(nn.Dropout(drop))\n        self.model = nn.Sequential(*layers)\n    def forward(self, x): return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_c, out_c, drop=0.0):\n        super().__init__()\n        layers = [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False), nn.InstanceNorm2d(out_c), nn.ReLU(True)]\n        if drop: layers.append(nn.Dropout(drop))\n        self.model = nn.Sequential(*layers)\n    def forward(self, x, skip):\n        x = self.model(x)\n        # Resize if needed (shape safety)\n        if x.shape[2:] != skip.shape[2:]:\n             x = torch.nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear')\n        return torch.cat((x, skip), 1)\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, in_c=1, out_c=3):\n        super().__init__()\n        self.d1=UNetDown(in_c,64,norm=False); self.d2=UNetDown(64,128); self.d3=UNetDown(128,256)\n        self.d4=UNetDown(256,512,drop=0.5); self.d5=UNetDown(512,512,drop=0.5); self.d6=UNetDown(512,512,drop=0.5)\n        self.d7=UNetDown(512,512,drop=0.5); self.d8=UNetDown(512,512,norm=False,drop=0.5)\n        self.u1=UNetUp(512,512,drop=0.5); self.u2=UNetUp(1024,512,drop=0.5); self.u3=UNetUp(1024,512,drop=0.5)\n        self.u4=UNetUp(1024,512,drop=0.5); self.u5=UNetUp(1024,256); self.u6=UNetUp(512,128); self.u7=UNetUp(256,64)\n        self.final=nn.Sequential(nn.Upsample(scale_factor=2), nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(128,out_c,4,padding=1), nn.Tanh())\n    def forward(self, x):\n        d1=self.d1(x); d2=self.d2(d1); d3=self.d3(d2); d4=self.d4(d3); d5=self.d5(d4); d6=self.d6(d5); d7=self.d7(d6); d8=self.d8(d7)\n        u1=self.u1(d8,d7); u2=self.u2(u1,d6); u3=self.u3(u2,d5); u4=self.u4(u3,d4); u5=self.u5(u4,d3); u6=self.u6(u5,d2); u7=self.u7(u6,d1)\n        return self.final(u7)\n\nclass PatchGANDiscriminator(nn.Module):\n    def __init__(self, in_c=1):\n        super().__init__()\n        def block(i, o, n=True):\n            l = [nn.Conv2d(i, o, 4, 2, 1)]; \n            if n: l.append(nn.InstanceNorm2d(o)); \n            l.append(nn.LeakyReLU(0.2, True)); return l\n        self.model = nn.Sequential(*block(in_c+3,64,False), *block(64,128), *block(128,256), *block(256,512), \n                                   nn.ZeroPad2d((1,0,1,0)), nn.Conv2d(512,1,4,padding=1,bias=False))\n    def forward(self, a, b): return self.model(torch.cat((a, b), 1))\n\ndef weights_init_normal(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight.data, 1.0, 0.02); torch.nn.init.constant_(m.bias.data, 0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:34:51.760248Z","iopub.execute_input":"2026-01-19T10:34:51.760606Z","iopub.status.idle":"2026-01-19T10:34:51.779479Z","shell.execute_reply.started":"2026-01-19T10:34:51.760569Z","shell.execute_reply":"2026-01-19T10:34:51.778511Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 5. Professional Training (Styled)","metadata":{}},{"cell_type":"code","source":"\ndef save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D):\n    path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n    torch.save({'epoch': epoch, 'G': generator.state_dict(), 'D': discriminator.state_dict(), 'optG': optimizer_G.state_dict(), 'optD': optimizer_D.state_dict()}, path)\n    print(f\"Saved: {path}\")\n\ndef load_checkpoint(generator, discriminator, optimizer_G, optimizer_D):\n    files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"checkpoint\")], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n    if not files: return 0\n    # Try catch for corrupted files\n    try:\n        cp = torch.load(os.path.join(CHECKPOINT_DIR, files[-1]), map_location=DEVICE)\n        generator.load_state_dict(cp['G']); discriminator.load_state_dict(cp['D']); optimizer_G.load_state_dict(cp['optG']); optimizer_D.load_state_dict(cp['optD'])\n        print(f\"Resumed epoch {cp['epoch']}\"); return cp['epoch'] + 1\n    except:\n        return 0\n\ngenerator = UNetGenerator().to(DEVICE); discriminator = PatchGANDiscriminator().to(DEVICE)\ngenerator.apply(weights_init_normal); discriminator.apply(weights_init_normal)\ncriterion_GAN = nn.BCEWithLogitsLoss().to(DEVICE); criterion_L1 = nn.L1Loss().to(DEVICE)\noptimizer_G = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\nscheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=EPOCHS)\nscheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=EPOCHS)\n\ndataloader = DataLoader(SynesthesiaDataset(size=4000, style_file=\"/kaggle/input/stylee/smooth-stucco-wall.jpg\"), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nscaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None\n\nSTART_EPOCH = load_checkpoint(generator, discriminator, optimizer_G, optimizer_D)\n\nfor epoch in range(START_EPOCH, EPOCHS):\n    for i, (spec, real_b) in enumerate(dataloader):\n        real_a, real_b = spec.to(DEVICE), real_b.to(DEVICE)\n        \n        # Train G\n        optimizer_G.zero_grad()\n        with torch.amp.autocast('cuda') if scaler else torch.no_grad():\n             if scaler:\n                fake_b = generator(real_a)\n                pred_fake = discriminator(fake_b, real_a)\n                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n             else:\n                fake_b = generator(real_a)\n                pred_fake = discriminator(fake_b, real_a)\n                loss_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake)) + LAMBDA_L1 * criterion_L1(fake_b, real_b)\n        \n        if scaler: scaler.scale(loss_G).backward(); scaler.step(optimizer_G); scaler.update()\n        else: loss_G.backward(); optimizer_G.step()\n\n        # Train D\n        optimizer_D.zero_grad()\n        with torch.amp.autocast('cuda') if scaler else torch.no_grad():\n             if scaler:\n                pred_real = discriminator(real_b, real_a)\n                pred_fake = discriminator(fake_b.detach(), real_a)\n                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n             else:\n                pred_real = discriminator(real_b, real_a)\n                pred_fake = discriminator(fake_b.detach(), real_a)\n                loss_D = 0.5 * (criterion_GAN(pred_real, torch.ones_like(pred_real)) + criterion_GAN(pred_fake, torch.zeros_like(pred_fake)))\n\n        if scaler: scaler.scale(loss_D).backward(); scaler.step(optimizer_D); scaler.update()\n        else: loss_D.backward(); optimizer_D.step()\n\n        if i % 100 == 0: print(f\"E{epoch} B{i} L_D:{loss_D.item():.3f} L_G:{loss_G.item():.3f}\")\n\n    scheduler_G.step(); scheduler_D.step()\n    if (epoch+1) % 10 == 0: save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D)\n    if (epoch+1) % 5 == 0:\n        generator.eval()\n        with torch.no_grad():\n            f = generator(real_a[:3])\n            plt.figure(figsize=(9,9))\n            for k in range(3):\n                plt.subplot(3,3,k*3+1); plt.imshow(real_a[k].cpu().squeeze(),cmap='magma',origin='lower'); plt.axis('off')\n                plt.subplot(3,3,k*3+2); plt.imshow(real_b[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n                plt.subplot(3,3,k*3+3); plt.imshow(f[k].cpu().permute(1,2,0)*0.5+0.5); plt.axis('off')\n            plt.show()\n        generator.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T10:34:51.780593Z","iopub.execute_input":"2026-01-19T10:34:51.780889Z"}},"outputs":[{"name":"stdout","text":"Loading custom style texture from /kaggle/input/stylee/smooth-stucco-wall.jpg\nE0 B0 L_D:0.910 L_G:88.405\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 6. Kinetic 3D Experience (Synthetic Test)","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchaudio\nimport torchaudio.transforms as T\nimport cv2\nfrom moviepy.editor import ImageSequenceClip, AudioFileClip\n\n# 1. CHARGEMENT DU CHECKPOINT\ncheckpoint_path = \"/kaggle/input/checkpoints/checkpoint_epoch_199.pth\"\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    generator.eval()\n    print(f\"‚úÖ Mod√®le charg√© avec succ√®s depuis l'√©poque 199\")\nelse:\n    print(\"‚ùå Checkpoint introuvable, v√©rifiez le chemin.\")\n\n# 2. FONCTION DE G√âN√âRATION CORRIG√âE\ndef generate_kinetic_video_fixed(audio_path, output_path=\"final_synesthesia_demo.mp4\", fps=30):\n    print(f\"üöÄ Rendu en cours pour : {audio_path}...\")\n    \n    waveform, sr = torchaudio.load(audio_path)\n    if sr != SAMPLE_RATE: \n        waveform = T.Resample(sr, SAMPLE_RATE)(waveform)\n    if waveform.shape[0] > 1:\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n    window_samples = int(2.0 * SAMPLE_RATE)\n    step_samples = int(SAMPLE_RATE / fps)\n    temp_frames = []\n    \n    # On limite √† 60 secondes pour la d√©mo si besoin\n    max_samples = min(waveform.shape[1], 60 * SAMPLE_RATE)\n\n    with torch.no_grad():\n        for start in range(0, max_samples - window_samples, step_samples):\n            chunk = waveform[:, start:start+window_samples]\n            # Utilisation de votre normaliseur\n            spec = normalizer.transform(chunk) \n            fake_tensor = generator(spec.unsqueeze(0).to(DEVICE))\n            \n            # Post-process image\n            img_np = np.clip((fake_tensor.squeeze().permute(1,2,0).cpu().numpy()*0.5+0.5), 0, 1)\n            \n            # Rendu Graphique\n            h_map = cv2.resize(img_np, (64, 64))\n            Z = np.dot(h_map[..., :3], [0.299, 0.587, 0.114])\n            X, Y = np.meshgrid(np.linspace(0,1,64), np.linspace(0,1,64))\n            \n            fig = plt.figure(figsize=(12, 6), dpi=100)\n            ax2d = fig.add_subplot(1, 2, 1)\n            ax2d.imshow(img_np)\n            ax2d.axis('off')\n            ax2d.set_title(\"Art Synesth√©sique (IA)\")\n            \n            ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n            surf = ax3d.plot_surface(X, Y, Z, facecolors=h_map, linewidth=0, antialiased=True)\n            ax3d.set_zlim(0, 1)\n            ax3d.view_init(elev=30, azim=start/500) # Rotation fluide\n            ax3d.axis('off')\n            ax3d.set_title(\"Topographie du Timbre\")\n            \n            # FIX : Utilisation du buffer correct\n            fig.canvas.draw()\n            rgba_buffer = fig.canvas.buffer_rgba()\n            frame = np.array(rgba_buffer)[:, :, :3] # On garde RGB, on enl√®ve Alpha\n            temp_frames.append(frame)\n            plt.close(fig)\n            \n            if len(temp_frames) % 50 == 0:\n                print(f\"üé• Frames g√©n√©r√©es : {len(temp_frames)}\")\n\n    if temp_frames:\n        clip = ImageSequenceClip(temp_frames, fps=fps)\n        audio_clip = AudioFileClip(audio_path).subclip(0, len(temp_frames)/fps)\n        final_video = clip.set_audio(audio_clip)\n        final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n        print(f\"‚ú® Vid√©o finale pr√™te : {output_path}\")\n\n# 3. LANCEMENT DU TEST BEETHOVEN\nbeethoven_path = \"/kaggle/input/beetho/beethoven-moonlight-sonata-1st-movement_x8ZwpaRE.wav\"\nif os.path.exists(beethoven_path):\n    generate_kinetic_video_fixed(beethoven_path, \"beethoven_final_art.mp4\")\nelse:\n    print(\"Fichier Beethoven non trouv√©, lancement du test synth√©tique...\")\n    generate_kinetic_video_fixed(\"kinetics_test.wav\", \"test_art.mp4\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}