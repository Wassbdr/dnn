{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683},{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":14537342,"sourceType":"datasetVersion","datasetId":9284938},{"sourceId":14537360,"sourceType":"datasetVersion","datasetId":9284950},{"sourceId":4508855,"sourceType":"kernelVersion"},{"sourceId":12703386,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pix2Pix - Image-to-Image Translation (Ultimate v4)\n\nCe notebook implémente une version **très avancée** de Pix2Pix/GAN pour une qualité maximale.\n\n## Nouveautés v4 (High Fidelity) :\n1.  **Perceptual Loss (VGG19)** : Au lieu de comparer les pixels (flou), on compare les \"features\" extraites par un VGG pré-entraîné. Cela force le générateur à respecter la structure et la texture.\n2.  **Replay Buffer** : Le discriminateur s'entraîne sur un historique d'images générées pour éviter les oscillations.\n3.  **ResNet Generator (9 blocks)** : Architecture plus profonde et puissante que le U-Net classique (inspiré de CycleGAN/Pix2PixHD) pour une meilleure propagation de l'information.\n4.  **TTUR** : Learning Rate différent pour le Générateur (2e-4) et le Discriminateur (4e-4).\n5.  **Optimisations** : Kaiming Init, Num Workers = 4.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.292412Z","iopub.execute_input":"2026-01-18T17:47:24.293021Z","iopub.status.idle":"2026-01-18T17:47:24.297823Z","shell.execute_reply.started":"2026-01-18T17:47:24.292983Z","shell.execute_reply":"2026-01-18T17:47:24.297091Z"}},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":"## 1. Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # TTUR: Deux Learning Rates differents\n    LR_G = 2e-4\n    LR_D = 4e-4\n    BATCH_SIZE = 1 \n    NUM_WORKERS = 4 # Optimisation CPU\n    IMAGE_SIZE = 256\n    CHANNELS_IMG = 3\n    L1_LAMBDA = 10 \n    VGG_LAMBDA = 10 # Poids de la Perceptual Loss\n    NUM_EPOCHS = 7\n    LOAD_MODEL = False\n    SAVE_MODEL = True\n    CHECKPOINT_DISC = \"disc_v4.pth.tar\"\n    CHECKPOINT_GEN = \"gen_v4.pth.tar\"\n    \n    TRAIN_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/train\"\n    VAL_DIR = \"/kaggle/input/pix2pix-dataset/edges2shoes/edges2shoes/val\"\n    \n    TRAIN_SIZE_LIMIT = None \n    \nprint(f\"Device used: {Config.DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.299680Z","iopub.execute_input":"2026-01-18T17:47:24.299981Z","iopub.status.idle":"2026-01-18T17:47:24.314084Z","shell.execute_reply.started":"2026-01-18T17:47:24.299951Z","shell.execute_reply":"2026-01-18T17:47:24.313435Z"}},"outputs":[{"name":"stdout","text":"Device used: cuda\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"## 2. Pipeline de Données","metadata":{}},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    def __init__(self, root_dir):\n        self.root_dir = root_dir\n        if os.path.exists(root_dir):\n            self.list_files = os.listdir(root_dir)\n        else:\n            print(f\"Attention: Le dossier {root_dir} n'existe pas.\")\n            self.list_files = []\n\n    def __len__(self):\n        return len(self.list_files)\n\n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        try:\n            image = np.array(Image.open(img_path))\n        except:\n            return self.__getitem__(index + 1)\n\n        w = image.shape[1]\n        cutoff = w // 2\n        input_image = image[:, :cutoff, :] \n        target_image = image[:, cutoff:, :]\n\n        input_image = Image.fromarray(input_image)\n        target_image = Image.fromarray(target_image)\n        \n        # Augmentation plus agressive\n        resize = transforms.Resize((286, 286))\n        input_image = resize(input_image)\n        target_image = resize(target_image)\n        \n        i, j, h, w_crop = transforms.RandomCrop.get_params(\n            input_image, output_size=(Config.IMAGE_SIZE, Config.IMAGE_SIZE)\n        )\n        input_image = transforms.functional.crop(input_image, i, j, h, w_crop)\n        target_image = transforms.functional.crop(target_image, i, j, h, w_crop)\n        \n        if torch.rand(1) > 0.5:\n            input_image = transforms.functional.hflip(input_image)\n            target_image = transforms.functional.hflip(target_image)\n            \n        base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n        ])\n        \n        input_image = base_transform(input_image)\n        target_image = base_transform(target_image)\n\n        return input_image, target_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.314984Z","iopub.execute_input":"2026-01-18T17:47:24.315335Z","iopub.status.idle":"2026-01-18T17:47:24.326334Z","shell.execute_reply.started":"2026-01-18T17:47:24.315303Z","shell.execute_reply":"2026-01-18T17:47:24.325665Z"}},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":"## 3. Utilitaires Avancés : ReplayBuffer & VGG Loss","metadata":{}},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return torch.cat(to_return)\n\nclass VGGLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Utilisation de VGG19 pre-entraine, jusqu'a la couche ReLU5_4\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n        self.slice = nn.Sequential()\n        # On garde les 35 premieres couches (perceptual features)\n        for i in range(35):\n            self.slice.add_module(str(i), vgg[i])\n        self.slice.eval() # Freeze\n        for param in self.slice.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, y):\n        # Les images doivent etre denormalisees de [-1, 1] vers [0, 1] puis normalisees ImageNet si besoin\n        # Ici on suppose que le VGG est robuste, on passe juste les images.\n        x_vgg = self.slice(x)\n        y_vgg = self.slice(y)\n        loss = nn.MSELoss()(x_vgg, y_vgg)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.327592Z","iopub.execute_input":"2026-01-18T17:47:24.327990Z","iopub.status.idle":"2026-01-18T17:47:24.342478Z","shell.execute_reply.started":"2026-01-18T17:47:24.327932Z","shell.execute_reply":"2026-01-18T17:47:24.341861Z"}},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":"## 4. Architecture ResNet Generator (9 Blocks) & Discriminator","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(channels),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass ResNetGenerator(nn.Module):\n    def __init__(self, in_channels=3, features=64, num_residuals=9):\n        super().__init__()\n        # Initial Conv\n        model = [\n            nn.Conv2d(in_channels, features, kernel_size=7, padding=3, padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(features),\n            nn.ReLU(inplace=True),\n        ]\n        # Downsampling\n        in_features = features\n        for _ in range(2):\n            out_features = in_features * 2\n            model += [\n                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual Blocks\n        for _ in range(num_residuals):\n            model += [ResidualBlock(in_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features = in_features // 2\n            model += [\n                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output Layer\n        model += [\n            nn.Conv2d(features, 3, kernel_size=7, padding=3, padding_mode=\"reflect\"),\n            nn.Tanh(),\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        # Discriminator standart PatchGAN\n        self.initial = nn.Sequential(\n            nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            layers.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, feature, 4, stride=1 if feature == features[-1] else 2, padding=1, bias=False, padding_mode=\"reflect\"),\n                    nn.InstanceNorm2d(feature, affine=True),\n                    nn.LeakyReLU(0.2, inplace=True),\n                )\n            )\n            in_channels = feature\n\n        layers.append(\n            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n        )\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        x = self.initial(x)\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.392134Z","iopub.execute_input":"2026-01-18T17:47:24.392622Z","iopub.status.idle":"2026-01-18T17:47:24.403478Z","shell.execute_reply.started":"2026-01-18T17:47:24.392601Z","shell.execute_reply":"2026-01-18T17:47:24.402781Z"}},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":"## 5. Training Loop v4","metadata":{}},{"cell_type":"code","source":"def weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        nn.init.kaiming_normal_(m.weight, a=0.02, mode='fan_in', nonlinearity='leaky_relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight, 1.0, 0.02)\n        nn.init.constant_(m.bias, 0)\n\ndef train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, mse_loss, vgg_loss, g_scaler, d_scaler, replay_buffer):\n    loop = tqdm(loader, leave=True)\n\n    for idx, (x, y) in enumerate(loop):\n        x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n        \n        # --- Generator Forward ---\n        with torch.amp.autocast('cuda'):\n            y_fake = gen(x)\n            \n            # Generator Loss\n            D_fake = disc(x, y_fake)\n            G_gan_loss = mse_loss(D_fake, torch.ones_like(D_fake))\n            \n            G_l1_loss = l1_loss(y_fake, y) * Config.L1_LAMBDA\n            G_vgg_loss = vgg_loss(y_fake, y) * Config.VGG_LAMBDA\n            \n            G_loss = G_gan_loss + G_l1_loss + G_vgg_loss\n\n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # --- Discriminator Forward ---\n        with torch.amp.autocast('cuda'):\n            # Replay Buffer: On recupere un mix d'images fake actuelles et passees\n            y_fake_buffer = replay_buffer.push_and_pop(y_fake.detach())\n            \n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake_buffer)\n            \n            D_real_loss = mse_loss(D_real, torch.ones_like(D_real) * 0.9) # Label smoothing\n            D_fake_loss = mse_loss(D_fake, torch.zeros_like(D_fake))\n            \n            D_loss = (D_real_loss + D_fake_loss) * 0.5\n\n        opt_disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n\n        if idx % 10 == 0:\n            loop.set_postfix(D=D_loss.item(), G=G_loss.item(), VGG=G_vgg_loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.405194Z","iopub.execute_input":"2026-01-18T17:47:24.405450Z","iopub.status.idle":"2026-01-18T17:47:24.417175Z","shell.execute_reply.started":"2026-01-18T17:47:24.405431Z","shell.execute_reply":"2026-01-18T17:47:24.416613Z"}},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":"## 6. Main","metadata":{}},{"cell_type":"code","source":"def save_some_examples(gen, val_loader, epoch, folder):\n    x, y = next(iter(val_loader))\n    x, y = x.to(Config.DEVICE), y.to(Config.DEVICE)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5\n        x = x * 0.5 + 0.5\n        y = y * 0.5 + 0.5\n        save_image(y_fake, folder + f\"/gen_{epoch}.png\")\n        save_image(x, folder + f\"/input_{epoch}.png\")\n        save_image(y, folder + f\"/label_{epoch}.png\")\n    gen.train()\n\ndef main():\n    disc = Discriminator().to(Config.DEVICE)\n    gen = ResNetGenerator(num_residuals=9).to(Config.DEVICE)\n    \n    disc.apply(weights_init)\n    gen.apply(weights_init)\n    \n    # TTUR: LRs differents\n    opt_disc = optim.Adam(disc.parameters(), lr=Config.LR_D, betas=(0.5, 0.999))\n    opt_gen = optim.Adam(gen.parameters(), lr=Config.LR_G, betas=(0.5, 0.999))\n\n    scheduler_G = optim.lr_scheduler.StepLR(opt_gen, step_size=20, gamma=0.5)\n    scheduler_D = optim.lr_scheduler.StepLR(opt_disc, step_size=20, gamma=0.5)\n\n    MSE_LOSS = nn.MSELoss()\n    L1_LOSS = nn.L1Loss()\n    VGG_LOSS = VGGLoss().to(Config.DEVICE)\n\n    train_dataset = Pix2PixDataset(root_dir=Config.TRAIN_DIR)\n    \n    if Config.TRAIN_SIZE_LIMIT and len(train_dataset) > Config.TRAIN_SIZE_LIMIT:\n        indices = torch.randperm(len(train_dataset))[:Config.TRAIN_SIZE_LIMIT]\n        train_dataset = torch.utils.data.Subset(train_dataset, indices)\n\n    if len(train_dataset) > 0:\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=Config.NUM_WORKERS,\n            pin_memory=True,\n        )\n        val_dataset = Pix2PixDataset(root_dir=Config.VAL_DIR)\n        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False) if len(val_dataset) > 0 else train_loader\n\n        g_scaler = torch.amp.GradScaler('cuda')\n        d_scaler = torch.amp.GradScaler('cuda')\n        replay_buffer = ReplayBuffer()\n\n        for epoch in range(Config.NUM_EPOCHS):\n            print(f\"Epoch {epoch}/{Config.NUM_EPOCHS} | LR_G: {scheduler_G.get_last_lr()[0]:.6f}\")\n            train_fn(\n                disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, MSE_LOSS, VGG_LOSS, g_scaler, d_scaler, replay_buffer\n            )\n            \n            scheduler_G.step()\n            scheduler_D.step()\n            \n            if Config.SAVE_MODEL and epoch % 5 == 0:\n                save_some_examples(gen, val_loader, epoch, folder=\"evaluation\")\n                torch.save(gen.state_dict(), Config.CHECKPOINT_GEN)\n                torch.save(disc.state_dict(), Config.CHECKPOINT_DISC)\n    else:\n        print(\"No data found.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T17:47:24.417970Z","iopub.execute_input":"2026-01-18T17:47:24.418176Z","iopub.status.idle":"2026-01-18T23:13:44.469473Z","shell.execute_reply.started":"2026-01-18T17:47:24.418157Z","shell.execute_reply":"2026-01-18T23:13:44.467990Z"}},"outputs":[{"name":"stdout","text":"Epoch 0/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:55<00:00, 15.40it/s, D=0.00181, G=11.7, VGG=9.03] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:46<00:00, 15.44it/s, D=0.000111, G=23.3, VGG=19.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:44<00:00, 15.45it/s, D=0.000293, G=12.3, VGG=7.96]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:44<00:00, 15.45it/s, D=0.0002, G=13.7, VGG=11.3]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:43<00:00, 15.46it/s, D=2.27e-5, G=21.5, VGG=18.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49825/49825 [53:55<00:00, 15.40it/s, D=2.51e-5, G=11, VGG=8.85]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/7 | LR_G: 0.000200\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 3192/49825 [03:26<50:19, 15.45it/s, D=7.43e-5, G=10.7, VGG=8.78] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3331201200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/3331201200.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{Config.NUM_EPOCHS} | LR_G: {scheduler_G.get_last_lr()[0]:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             train_fn(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVGG_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n","\u001b[0;32m/tmp/ipykernel_55/3363771099.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc, gen, loader, opt_disc, opt_gen, l1_loss, mse_loss, vgg_loss, g_scaler, d_scaler, replay_buffer)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":95},{"cell_type":"markdown","source":"## 7. Inference & Custom Sketch","metadata":{}},{"cell_type":"code","source":"def predict_custom_sketch(image_path):\n    print(f\"--- Prediction sur {image_path} ---\")\n    gen = ResNetGenerator(num_residuals=9).to(Config.DEVICE)\n    if os.path.exists(Config.CHECKPOINT_GEN):\n        gen.load_state_dict(torch.load(Config.CHECKPOINT_GEN, map_location=Config.DEVICE))\n        gen.eval()\n    else:\n        print(\"Erreur: Pas de checkpoint.\")\n        return\n\n    if not os.path.exists(image_path):\n        print(\"Image introuvable\")\n        return\n        \n    img = Image.open(image_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n    ])\n    x = t(img).unsqueeze(0).to(Config.DEVICE)\n    with torch.no_grad():\n        res = gen(x).squeeze().cpu() * 0.5 + 0.5\n        plt.imshow(res.permute(1, 2, 0))\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T23:14:03.360978Z","iopub.execute_input":"2026-01-18T23:14:03.361673Z","iopub.status.idle":"2026-01-18T23:14:03.367220Z","shell.execute_reply.started":"2026-01-18T23:14:03.361644Z","shell.execute_reply":"2026-01-18T23:14:03.366586Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}